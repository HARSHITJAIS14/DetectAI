{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded:\n",
      "Base dataset path: /home/jivnesh/Harshit_Surge/dataset/eval_dataset\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION: Update these paths based on your environment\n",
    "# ============================================================================\n",
    "# Base path to the dataset directory\n",
    "BASE_DATASET_PATH = \"/home/jivnesh/Harshit_Surge/dataset/eval_dataset\"\n",
    "\n",
    "# Alternatively, you can use relative paths:\n",
    "# BASE_DATASET_PATH = Path(__file__).parent / \"dataset\" / \"eval_dataset\"\n",
    "\n",
    "# Ensure the base path exists\n",
    "if not os.path.exists(BASE_DATASET_PATH):\n",
    "    print(f\"Warning: Dataset path '{BASE_DATASET_PATH}' does not exist. Please update BASE_DATASET_PATH.\")\n",
    "\n",
    "# Define individual dataset file paths\n",
    "RAID_EVAL_PATH = os.path.join(BASE_DATASET_PATH, \"raid_eval.csv\")\n",
    "M4_EVAL_PATH = os.path.join(BASE_DATASET_PATH, \"m4_eval.csv\")\n",
    "CHEAT_EVAL_PATH = os.path.join(BASE_DATASET_PATH, \"cheat_eval.csv\")\n",
    "HC3_EVAL_PATH = os.path.join(BASE_DATASET_PATH, \"hc3_eval.csv\")\n",
    "MAGE_EVAL_PATH = os.path.join(BASE_DATASET_PATH, \"mage_eval.csv\")\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "print(f\"Base dataset path: {BASE_DATASET_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1610340/2475655846.py:3: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  raid_eval[\"label\"] = raid_eval[\"models\"].replace({\"human\": 0, \"ai\": 1})\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "raid_eval = pd.read_csv(RAID_EVAL_PATH)\n",
    "raid_eval[\"label\"] = raid_eval[\"models\"].replace({\"human\": 0, \"ai\": 1})\n",
    "m4_eval = pd.read_csv(M4_EVAL_PATH)\n",
    "cheat_eval = pd.read_csv(CHEAT_EVAL_PATH)\n",
    "hc3_eval = pd.read_csv(HC3_EVAL_PATH)\n",
    "mage_eval = pd.read_csv(MAGE_EVAL_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU 0: NVIDIA A100 80GB PCIe\n",
      "GPU 1: NVIDIA A100 80GB PCIe\n",
      "GPU 2: NVIDIA A100 80GB PCIe\n",
      "GPU 3: NVIDIA A100 80GB PCIe\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized encoder: 768 → 256\n",
      "Safe load failed for mlp_probe.pt: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n",
      "\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n",
      "\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n",
      "\tWeightsUnpickler error: Unsupported global: GLOBAL numpy.core.multiarray.scalar was not an allowed global by default. Please use `torch.serialization.add_safe_globals([numpy.core.multiarray.scalar])` or the `torch.serialization.safe_globals([numpy.core.multiarray.scalar])` context manager to allowlist this global if you trust this class/function.\n",
      "\n",
      "Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.\n",
      "Retrying with weights_only=False (trusted file only)\n",
      "Loaded encoder + probe | best_val_f1=0.9848541744899236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MLPDetect: 100%|██████████| 188/188 [03:39<00:00,  1.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating RAID Eval (n=12000)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Human       0.93      0.98      0.96      4000\n",
      "          AI       0.99      0.97      0.98      8000\n",
      "\n",
      "    accuracy                           0.97     12000\n",
      "   macro avg       0.96      0.97      0.97     12000\n",
      "weighted avg       0.97      0.97      0.97     12000\n",
      "\n",
      "Confusion Matrix:\n",
      " [[3936   64]\n",
      " [ 275 7725]]\n",
      "False Positive Rate: 0.0160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MLPDetect: 100%|██████████| 157/157 [02:18<00:00,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating M4 Eval (n=10000)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Human       0.51      0.99      0.68      5000\n",
      "          AI       0.82      0.07      0.12      5000\n",
      "\n",
      "    accuracy                           0.53     10000\n",
      "   macro avg       0.67      0.53      0.40     10000\n",
      "weighted avg       0.67      0.53      0.40     10000\n",
      "\n",
      "Confusion Matrix:\n",
      " [[4926   74]\n",
      " [4662  338]]\n",
      "False Positive Rate: 0.0148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MLPDetect: 100%|██████████| 157/157 [00:58<00:00,  2.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Cheat Eval (n=10000)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Human       1.00      0.94      0.97      5000\n",
      "          AI       0.95      1.00      0.97      5000\n",
      "\n",
      "    accuracy                           0.97     10000\n",
      "   macro avg       0.97      0.97      0.97     10000\n",
      "weighted avg       0.97      0.97      0.97     10000\n",
      "\n",
      "Confusion Matrix:\n",
      " [[4714  286]\n",
      " [  13 4987]]\n",
      "False Positive Rate: 0.0572\n",
      "\n",
      "Summary:\n",
      "       Dataset  Accuracy     FPR\n",
      "0   RAID Eval   0.97175  0.0160\n",
      "1     M4 Eval   0.52640  0.0148\n",
      "2  Cheat Eval   0.97010  0.0572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Environment & Imports\n",
    "# ============================================================\n",
    "\n",
    "# !pip install torchview\n",
    "# !pip install torchviz\n",
    "\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import warnings\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    ")\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ============================================================\n",
    "# Reproducibility & Device\n",
    "# ============================================================\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "\n",
    "# ============================================================\n",
    "# Style Contrastive Encoder\n",
    "# ============================================================\n",
    "\n",
    "class StyleContrastiveEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        base_model: str = \"microsoft/deberta-v3-base\",\n",
    "        embedding_dim: int = 256,\n",
    "        dropout: float = 0.1,\n",
    "        freeze_backbone: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.backbone = AutoModel.from_pretrained(base_model)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "\n",
    "        if freeze_backbone:\n",
    "            for p in self.backbone.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "        backbone_dim = self.backbone.config.hidden_size\n",
    "\n",
    "        self.projection_head = nn.Sequential(\n",
    "            nn.Linear(backbone_dim, backbone_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(backbone_dim // 2, embedding_dim),\n",
    "            nn.LayerNorm(embedding_dim),\n",
    "        )\n",
    "\n",
    "        print(f\"Initialized encoder: {backbone_dim} → {embedding_dim}\")\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.backbone(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "        )\n",
    "\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "        attn = attention_mask.unsqueeze(-1).float()\n",
    "\n",
    "        pooled = (hidden_states * attn).sum(1) / attn.sum(1)\n",
    "        emb = self.projection_head(pooled)\n",
    "        emb = F.normalize(emb, p=2, dim=1)\n",
    "\n",
    "        return emb\n",
    "\n",
    "# ============================================================\n",
    "# InfoNCE Loss (unchanged)\n",
    "# ============================================================\n",
    "\n",
    "class InfoNCELoss(nn.Module):\n",
    "    def __init__(self, temperature: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, anchor, positive, negative):\n",
    "        pos_sim = torch.sum(anchor * positive, dim=1) / self.temperature\n",
    "        neg_sim = torch.sum(anchor * negative, dim=1) / self.temperature\n",
    "\n",
    "        logits = torch.stack([pos_sim, neg_sim], dim=1)\n",
    "        labels = torch.zeros(anchor.size(0), dtype=torch.long, device=anchor.device)\n",
    "\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        acc = (pos_sim > neg_sim).float().mean()\n",
    "\n",
    "        return loss, acc\n",
    "\n",
    "# ============================================================\n",
    "# Embedding Extraction (Frozen Encoder)\n",
    "# ============================================================\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_embeddings(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    texts: List[str],\n",
    "    device,\n",
    "    batch_size: int = 64,\n",
    "    max_length: int = 512,\n",
    "):\n",
    "    model.eval()\n",
    "    encoder = model.module if hasattr(model, \"module\") else model\n",
    "\n",
    "    all_embs = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Extracting embeddings\"):\n",
    "        batch = texts[i : i + batch_size]\n",
    "        toks = tokenizer(\n",
    "            batch,\n",
    "            max_length=max_length,\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(device)\n",
    "\n",
    "        emb = encoder(toks[\"input_ids\"], toks[\"attention_mask\"])\n",
    "        all_embs.append(emb.cpu())\n",
    "\n",
    "    return torch.cat(all_embs, dim=0).numpy()\n",
    "\n",
    "# ============================================================\n",
    "# MLP Probe\n",
    "# ============================================================\n",
    "\n",
    "class MLPProbe(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        emb_dim: int = 256,\n",
    "        hidden_dim: int = 128,\n",
    "        num_classes: int = 2,\n",
    "        dropout: float = 0.2,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(emb_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# ============================================================\n",
    "# Safe Torch Loading (PyTorch ≥2.6 compatible)\n",
    "# ============================================================\n",
    "\n",
    "try:\n",
    "    torch.serialization.add_safe_globals([np.core.multiarray.scalar])\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "def safe_load(path: str, map_location):\n",
    "    try:\n",
    "        return torch.load(path, map_location=map_location)\n",
    "    except Exception as e:\n",
    "        print(f\"Safe load failed for {path}: {e}\")\n",
    "        print(\"Retrying with weights_only=False (trusted file only)\")\n",
    "        return torch.load(path, map_location=map_location, weights_only=False)\n",
    "\n",
    "# ============================================================\n",
    "# MLP-Based Style Detector\n",
    "# ============================================================\n",
    "\n",
    "class MLPStyleDetector:\n",
    "    def __init__(self, model_path: str, probe_path: str, device):\n",
    "        self.device = device\n",
    "\n",
    "        self.model = StyleContrastiveEncoder().to(device)\n",
    "        ckpt = safe_load(model_path, device)\n",
    "        enc_state = ckpt.get(\"model_state_dict\", ckpt)\n",
    "\n",
    "        if enc_state and next(iter(enc_state)).startswith(\"module.\"):\n",
    "            enc_state = {k[7:]: v for k, v in enc_state.items()}\n",
    "\n",
    "        self.model.load_state_dict(enc_state, strict=False)\n",
    "        self.model.eval()\n",
    "        self.tokenizer = self.model.tokenizer\n",
    "\n",
    "        probe_ckpt = safe_load(probe_path, device)\n",
    "        emb_dim = probe_ckpt[\"emb_dim\"]\n",
    "\n",
    "        self.probe = MLPProbe(emb_dim=emb_dim).to(device)\n",
    "        self.probe.load_state_dict(probe_ckpt[\"probe_state_dict\"])\n",
    "        self.probe.eval()\n",
    "\n",
    "        self.meta = probe_ckpt.get(\"meta\", {})\n",
    "        print(f\"Loaded encoder + probe | best_val_f1={self.meta.get('best_val_f1', '?')}\")\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def batch_predict(self, texts: List[str], batch_size: int = 64):\n",
    "        preds = []\n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc=\"MLPDetect\"):\n",
    "            batch = texts[i : i + batch_size]\n",
    "            toks = self.tokenizer(\n",
    "                batch,\n",
    "                max_length=512,\n",
    "                truncation=True,\n",
    "                padding=True,\n",
    "                return_tensors=\"pt\",\n",
    "            ).to(self.device)\n",
    "\n",
    "            emb = self.model(toks[\"input_ids\"], toks[\"attention_mask\"])\n",
    "            logits = self.probe(emb)\n",
    "            preds.extend(logits.argmax(dim=1).cpu().tolist())\n",
    "\n",
    "        return preds\n",
    "\n",
    "# ============================================================\n",
    "# Evaluation\n",
    "# ============================================================\n",
    "\n",
    "def evaluate_with_mlp(detector, df: pd.DataFrame, name: str, batch_size: int = 64):\n",
    "    texts = df[\"text\"].astype(str).tolist()\n",
    "    labels = df[\"label\"].tolist()\n",
    "\n",
    "    preds = detector.batch_predict(texts, batch_size=batch_size)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "\n",
    "    print(f\"\\nEvaluating {name} (n={len(df)})\")\n",
    "    print(classification_report(labels, preds, target_names=[\"Human\", \"AI\"], zero_division=0))\n",
    "\n",
    "    cm = confusion_matrix(labels, preds)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    fpr = fp / (fp + tn) if (fp + tn) else 0.0\n",
    "\n",
    "    print(\"Confusion Matrix:\\n\", cm)\n",
    "    print(f\"False Positive Rate: {fpr:.4f}\")\n",
    "\n",
    "    return {\"name\": name, \"accuracy\": acc, \"fpr\": fpr, \"cm\": cm}\n",
    "\n",
    "# ============================================================\n",
    "# Main Execution\n",
    "# ============================================================\n",
    "\n",
    "MODEL_PATH = \"best_style_model.pt\"\n",
    "PROBE_PATH = \"mlp_probe.pt\"\n",
    "\n",
    "if os.path.exists(MODEL_PATH) and os.path.exists(PROBE_PATH):\n",
    "    detector = MLPStyleDetector(MODEL_PATH, PROBE_PATH, device)\n",
    "    results = []\n",
    "\n",
    "    if \"raid_eval\" in globals():\n",
    "        results.append(evaluate_with_mlp(detector, raid_eval, \"RAID Eval\"))\n",
    "    if \"m4_eval\" in globals():\n",
    "        results.append(evaluate_with_mlp(detector, m4_eval, \"M4 Eval\"))\n",
    "    if \"cheat_eval\" in globals():\n",
    "        results.append(evaluate_with_mlp(detector, cheat_eval, \"Cheat Eval\"))\n",
    "    if \"hc3_eval\" in globals():\n",
    "        results.append(evaluate_with_mlp(detector, hc3_eval, \"HC3 Eval\"))\n",
    "    if \"mage_eval\" in globals():\n",
    "        results.append(evaluate_with_mlp(detector, mage_eval, \"MAGE Eval\"))\n",
    "\n",
    "    if results:\n",
    "        summary = pd.DataFrame(\n",
    "            [{\"Dataset\": r[\"name\"], \"Accuracy\": r[\"accuracy\"], \"FPR\": r[\"fpr\"]} for r in results]\n",
    "        )\n",
    "        print(\"\\nSummary:\\n\", summary)\n",
    "else:\n",
    "    print(\"Missing best_style_model.pt or mlp_probe.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True, GPU count: 4\n",
      "Loading and preparing the dataset (assumes raid_eval, m4_eval, cheat_eval exist)...\n",
      "\n",
      "--- Starting Binoculars Evaluation (labels: 0=human, 1=ai) ---\n",
      "Initializing Binoculars (robust multi-GPU loader)...\n",
      "Attempting to load models with string-keyed max_memory map...\n",
      "max_memory map (string keys): {'cuda:0': '79037MB', 'cuda:1': '79037MB', 'cuda:2': '79037MB', 'cuda:3': '79037MB', 'cpu': '30000MB'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers wants integer device keys — retrying with integer-keyed max_memory map...\n",
      "max_memory map (int keys): {0: '79037MB', 1: '79037MB', 2: '79037MB', 3: '79037MB', 'cpu': '30000MB'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:36<00:00, 18.07s/it]\n",
      "\n",
      "WARNING: You are currently loading Falcon using legacy code contained in the model repository. Falcon has now been fully ported into the Hugging Face transformers library. For the most up-to-date and high-performance version of the Falcon model code, please update to the latest version of transformers and then load the model without the trust_remote_code=True argument.\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:36<00:00, 18.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binoculars initialized successfully.\n",
      "\n",
      "Running predictions on 12000 samples in batches of 32...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|██████████| 375/375 [29:08<00:00,  4.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "      Binoculars Classification Statistics\n",
      "==================================================\n",
      "\n",
      "--- Classification Report (labels 0/1) ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6865    0.8912    0.7756      4000\n",
      "           1     0.9361    0.7965    0.8607      8000\n",
      "\n",
      "    accuracy                         0.8281     12000\n",
      "   macro avg     0.8113    0.8439    0.8181     12000\n",
      "weighted avg     0.8529    0.8281    0.8323     12000\n",
      "\n",
      "\n",
      "Overall Accuracy: 0.8281\n",
      "ROC AUC Score: 0.9301\n",
      "\n",
      "--- Confusion Matrix (rows=true, cols=pred) ---\n",
      "[[3565  435]\n",
      " [1628 6372]]\n",
      "\n",
      "False Positive Rate (human misclassified as ai): 0.1087\n",
      "\n",
      "==================================================\n",
      "\n",
      "--- Starting Binoculars Evaluation (labels: 0=human, 1=ai) ---\n",
      "Initializing Binoculars (robust multi-GPU loader)...\n",
      "Attempting to load models with string-keyed max_memory map...\n",
      "max_memory map (string keys): {'cuda:0': '79037MB', 'cuda:1': '79037MB', 'cuda:2': '79037MB', 'cuda:3': '79037MB', 'cpu': '30000MB'}\n",
      "Transformers wants integer device keys — retrying with integer-keyed max_memory map...\n",
      "max_memory map (int keys): {0: '79037MB', 1: '79037MB', 2: '79037MB', 3: '79037MB', 'cpu': '30000MB'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:31<00:00, 15.82s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:21<00:00, 10.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binoculars initialized successfully.\n",
      "\n",
      "Running predictions on 10000 samples in batches of 32...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  52%|█████▏    | 163/313 [12:01<11:02,  4.42s/it]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from typing import Union, List\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, classification_report, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# --- Pre-computation Setup & Warnings ---\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "# --- Metric Functions (UNCHANGED) ---\n",
    "ce_loss_fn = torch.nn.CrossEntropyLoss(reduction=\"none\")\n",
    "softmax_fn = torch.nn.Softmax(dim=-1)\n",
    "\n",
    "def perplexity(encoding: transformers.BatchEncoding, logits: torch.Tensor, median: bool = False, temperature: float = 1.0):\n",
    "    shifted_logits = logits[..., :-1, :].contiguous() / temperature\n",
    "    shifted_labels = encoding.input_ids[..., 1:].contiguous()\n",
    "    shifted_attention_mask = encoding.attention_mask[..., 1:].contiguous()\n",
    "    if median:\n",
    "        ce_nan = (ce_loss_fn(shifted_logits.transpose(1, 2), shifted_labels).masked_fill(~shifted_attention_mask.bool(), float(\"nan\")))\n",
    "        ppl = np.nanmedian(ce_nan.cpu().float().numpy(), 1)\n",
    "    else:\n",
    "        ppl = (ce_loss_fn(shifted_logits.transpose(1, 2), shifted_labels) * shifted_attention_mask).sum(1) / shifted_attention_mask.sum(1)\n",
    "        ppl = ppl.to(\"cpu\").float().numpy()\n",
    "    return ppl\n",
    "\n",
    "def entropy(p_logits: torch.Tensor, q_logits: torch.Tensor, encoding: transformers.BatchEncoding, pad_token_id: int, median: bool = False, sample_p: bool = False, temperature: float = 1.0):\n",
    "    vocab_size = p_logits.shape[-1]\n",
    "    total_tokens_available = q_logits.shape[-2]\n",
    "    p_scores, q_scores = p_logits / temperature, q_logits / temperature\n",
    "    p_proba = softmax_fn(p_scores).view(-1, vocab_size)\n",
    "    if sample_p:\n",
    "        p_proba = torch.multinomial(p_proba.view(-1, vocab_size), replacement=True, num_samples=1).view(-1)\n",
    "    q_scores = q_scores.view(-1, vocab_size)\n",
    "    ce = ce_loss_fn(input=q_scores, target=p_proba).view(-1, total_tokens_available)\n",
    "    padding_mask = (encoding.input_ids != pad_token_id).type(torch.uint8)\n",
    "    if median:\n",
    "        ce_nan = ce.masked_fill(~padding_mask.bool(), float(\"nan\"))\n",
    "        agg_ce = np.nanmedian(ce_nan.cpu().float().numpy(), 1)\n",
    "    else:\n",
    "        agg_ce = (((ce * padding_mask).sum(1) / padding_mask.sum(1)).to(\"cpu\").float().numpy())\n",
    "    return agg_ce\n",
    "\n",
    "# --- Huggingface token (if any) ---\n",
    "huggingface_config = {\"TOKEN\": os.environ.get(\"HF_TOKEN\", None)}\n",
    "\n",
    "# NOTE: The threshold is based on the original Falcon models.\n",
    "BINOCULARS_ACCURACY_THRESHOLD = 0.9015310749276843\n",
    "\n",
    "# --- Device check ---\n",
    "if not torch.cuda.is_available():\n",
    "    print(\"Warning: CUDA not available. This code expects 4 GPUs for best performance.\")\n",
    "CUDA_DEVICE_COUNT = torch.cuda.device_count()\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}, GPU count: {CUDA_DEVICE_COUNT}\")\n",
    "\n",
    "# --- Utilities to build max_memory map for 4 x A100 (80GB) ---\n",
    "def build_max_memory_map_for_a100(reserve_mb_per_gpu: int = 2000):\n",
    "    \"\"\"\n",
    "    Build a max_memory mapping tuned for 4x A100 80GB.\n",
    "    reserve_mb_per_gpu - memory left free on each GPU for activations/tensors.\n",
    "    \"\"\"\n",
    "    max_memory = {}\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        total_bytes = torch.cuda.get_device_properties(i).total_memory\n",
    "        total_mb = total_bytes // (1024 * 1024)\n",
    "        avail_mb = max(0, total_mb - reserve_mb_per_gpu)\n",
    "        max_memory[f\"cuda:{i}\"] = f\"{avail_mb}MB\"\n",
    "    # CPU budget for offloading\n",
    "    max_memory[\"cpu\"] = \"30000MB\"\n",
    "    return max_memory\n",
    "\n",
    "# --- Binoculars Class (tuned for 4xA100) ---\n",
    "class Binoculars(object):\n",
    "    def __init__(self,\n",
    "                 observer_name_or_path: str = \"tiiuae/falcon-7b\",\n",
    "                 performer_name_or_path: str = \"tiiuae/falcon-7b-instruct\",\n",
    "                 use_bfloat16: bool = True,\n",
    "                 max_token_observed: int = 512,\n",
    "                 reserve_mb_per_gpu: int = 2000,\n",
    "                 explicit_split: bool = False):\n",
    "        \"\"\"\n",
    "        Robust loader that retries with integer-keyed max_memory if transformers complains\n",
    "        'Device cuda:0 is not recognized, available devices are integers(...)'.\n",
    "        \"\"\"\n",
    "        print(\"Initializing Binoculars (robust multi-GPU loader)...\")\n",
    "        self.threshold = BINOCULARS_ACCURACY_THRESHOLD\n",
    "        dtype = torch.bfloat16 if use_bfloat16 else torch.float32\n",
    "        hf_token = huggingface_config.get(\"TOKEN\", None)\n",
    "        auth_args = {\"use_auth_token\": hf_token} if hf_token else {}\n",
    "\n",
    "        # build both styles of max_memory maps\n",
    "        n_gpus = torch.cuda.device_count() if torch.cuda.is_available() else 0\n",
    "        max_memory_str = {}\n",
    "        max_memory_int = {}\n",
    "        for i in range(n_gpus):\n",
    "            total_bytes = torch.cuda.get_device_properties(i).total_memory\n",
    "            total_mb = total_bytes // (1024 * 1024)\n",
    "            avail_mb = max(0, total_mb - reserve_mb_per_gpu)\n",
    "            max_memory_str[f\"cuda:{i}\"] = f\"{avail_mb}MB\"\n",
    "            max_memory_int[i] = f\"{avail_mb}MB\"\n",
    "        max_memory_str[\"cpu\"] = \"30000MB\"\n",
    "        max_memory_int[\"cpu\"] = \"30000MB\"\n",
    "\n",
    "        def try_from_pretrained(name_or_path, device_map, max_memory_map):\n",
    "            \"\"\"Wrap from_pretrained to show clearer errors.\"\"\"\n",
    "            return AutoModelForCausalLM.from_pretrained(\n",
    "                name_or_path,\n",
    "                device_map=device_map,\n",
    "                max_memory=max_memory_map,\n",
    "                torch_dtype=dtype,\n",
    "                trust_remote_code=True,\n",
    "                **auth_args\n",
    "            )\n",
    "\n",
    "        try:\n",
    "            if explicit_split and n_gpus >= 4:\n",
    "                # Create explicit split maps (both styles)\n",
    "                max_memory_obs_str = {\"cuda:0\": max_memory_str.get(\"cuda:0\", \"0MB\"),\n",
    "                                      \"cuda:1\": max_memory_str.get(\"cuda:1\", \"0MB\"),\n",
    "                                      \"cuda:2\": \"2000MB\", \"cuda:3\": \"2000MB\", \"cpu\": \"30000MB\"}\n",
    "                max_memory_perf_str = {\"cuda:0\": \"2000MB\", \"cuda:1\": \"2000MB\",\n",
    "                                       \"cuda:2\": max_memory_str.get(\"cuda:2\", \"0MB\"),\n",
    "                                       \"cuda:3\": max_memory_str.get(\"cuda:3\", \"0MB\"),\n",
    "                                       \"cpu\": \"30000MB\"}\n",
    "\n",
    "                max_memory_obs_int = {0: max_memory_int.get(0, \"0MB\"),\n",
    "                                      1: max_memory_int.get(1, \"0MB\"),\n",
    "                                      2: \"2000MB\", 3: \"2000MB\", \"cpu\": \"30000MB\"}\n",
    "                max_memory_perf_int = {0: \"2000MB\", 1: \"2000MB\",\n",
    "                                       2: max_memory_int.get(2, \"0MB\"),\n",
    "                                       3: max_memory_int.get(3, \"0MB\"),\n",
    "                                       \"cpu\": \"30000MB\"}\n",
    "\n",
    "                # Try string-keyed first, then fallback to int-keyed if needed\n",
    "                try:\n",
    "                    print(\"Loading observer (explicit split) with string-keyed max_memory...\")\n",
    "                    self.observer_model = try_from_pretrained(observer_name_or_path, device_map=\"auto\", max_memory_map=max_memory_obs_str)\n",
    "                    print(\"Loading performer (explicit split) with string-keyed max_memory...\")\n",
    "                    self.performer_model = try_from_pretrained(performer_name_or_path, device_map=\"auto\", max_memory_map=max_memory_perf_str)\n",
    "                    used_int_keys = False\n",
    "                except Exception as e:\n",
    "                    msg = str(e)\n",
    "                    if \"Device cuda:0 is not recognized\" in msg or \"available devices are integers\" in msg:\n",
    "                        print(\"Detected device-keying mismatch; retrying explicit split with integer-keyed max_memory...\")\n",
    "                        self.observer_model = try_from_pretrained(observer_name_or_path, device_map=\"auto\", max_memory_map=max_memory_obs_int)\n",
    "                        self.performer_model = try_from_pretrained(performer_name_or_path, device_map=\"auto\", max_memory_map=max_memory_perf_int)\n",
    "                        used_int_keys = True\n",
    "                    else:\n",
    "                        raise\n",
    "\n",
    "                # Set preferred input devices (string style for torch.device)\n",
    "                self.obs_input_device = \"cuda:0\"\n",
    "                self.perf_input_device = \"cuda:2\"\n",
    "\n",
    "            else:\n",
    "                # Auto-shard across GPUs: try string-keyed map first, fallback to integer-keyed map.\n",
    "                try:\n",
    "                    print(\"Attempting to load models with string-keyed max_memory map...\")\n",
    "                    print(f\"max_memory map (string keys): {max_memory_str}\")\n",
    "                    self.observer_model = try_from_pretrained(observer_name_or_path, device_map=\"auto\", max_memory_map=max_memory_str)\n",
    "                    self.performer_model = try_from_pretrained(performer_name_or_path, device_map=\"auto\", max_memory_map=max_memory_str)\n",
    "                    used_int_keys = False\n",
    "                except Exception as e:\n",
    "                    msg = str(e)\n",
    "                    if \"Device cuda:0 is not recognized\" in msg or \"available devices are integers\" in msg:\n",
    "                        print(\"Transformers wants integer device keys — retrying with integer-keyed max_memory map...\")\n",
    "                        print(f\"max_memory map (int keys): {max_memory_int}\")\n",
    "                        self.observer_model = try_from_pretrained(observer_name_or_path, device_map=\"auto\", max_memory_map=max_memory_int)\n",
    "                        self.performer_model = try_from_pretrained(performer_name_or_path, device_map=\"auto\", max_memory_map=max_memory_int)\n",
    "                        used_int_keys = True\n",
    "                    else:\n",
    "                        raise\n",
    "\n",
    "                # choose default input devices\n",
    "                self.obs_input_device = \"cuda:0\" if torch.cuda.is_available() and n_gpus > 0 else \"cpu\"\n",
    "                self.perf_input_device = \"cuda:0\" if torch.cuda.is_available() and n_gpus > 0 else \"cpu\"\n",
    "\n",
    "            # finalize\n",
    "            self.observer_model.eval()\n",
    "            self.performer_model.eval()\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(observer_name_or_path, **auth_args)\n",
    "            if not self.tokenizer.pad_token:\n",
    "                self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "            self.max_token_observed = max_token_observed\n",
    "            print(\"Binoculars initialized successfully.\")\n",
    "        except Exception as e:\n",
    "            print(\"Error during model initialization (final):\", e)\n",
    "            raise\n",
    "\n",
    "\n",
    "    def _tokenize(self, batch: list[str]) -> transformers.BatchEncoding:\n",
    "        # tokenization on CPU; we will move encodings to GPU devices before model call\n",
    "        return self.tokenizer(batch, return_tensors=\"pt\", padding=\"longest\", truncation=True, max_length=self.max_token_observed, return_token_type_ids=False)\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def _get_logits(self, encodings: transformers.BatchEncoding) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Send the encodings to each model's preferred input device.\n",
    "        (With HF sharded models, sending to cuda:0 is typical; explicit_split uses different devices.)\n",
    "        \"\"\"\n",
    "        # Move encodings to observer input device\n",
    "        observer_device = torch.device(self.obs_input_device) if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "        performer_device = torch.device(self.perf_input_device) if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "        # .to(device) works on BatchEncoding\n",
    "        obs_enc = {k: v.to(observer_device) for k, v in encodings.items()}\n",
    "        perf_enc = {k: v.to(performer_device) for k, v in encodings.items()}\n",
    "\n",
    "        observer_out = self.observer_model(**obs_enc)\n",
    "        performer_out = self.performer_model(**perf_enc)\n",
    "\n",
    "        # logits (shape: batch, seq_len, vocab)\n",
    "        observer_logits = observer_out.logits\n",
    "        performer_logits = performer_out.logits\n",
    "\n",
    "        # synchronize if CUDA\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "        return observer_logits, performer_logits\n",
    "\n",
    "    def compute_score(self, input_text: Union[str, List[str]]) -> Union[float, List[float]]:\n",
    "        batch = [input_text] if isinstance(input_text, str) else input_text\n",
    "        encodings = self._tokenize(batch)\n",
    "        observer_logits, performer_logits = self._get_logits(encodings)\n",
    "        # we send encodings to performer device for ppl calculation to ensure mask alignments\n",
    "        # choose the performer device for perplexity computation (works fine)\n",
    "        perf_device = torch.device(self.perf_input_device) if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "        ppl_val = perplexity(encodings.to(perf_device), performer_logits)\n",
    "        x_ppl_val = entropy(observer_logits.to(observer_logits.device), performer_logits.to(performer_logits.device), encodings.to(perf_device), self.tokenizer.pad_token_id)\n",
    "        binoculars_scores = ppl_val / x_ppl_val\n",
    "        return binoculars_scores.tolist()[0] if isinstance(input_text, str) else binoculars_scores.tolist()\n",
    "\n",
    "# --- Evaluation Function (MODIFIED to allow big batch sizes) ---\n",
    "def evaluate_on_dataframe(df: pd.DataFrame, batch_size: int = 32):\n",
    "    print(\"\\n--- Starting Binoculars Evaluation (labels: 0=human, 1=ai) ---\")\n",
    "    try:\n",
    "        binoculars = Binoculars()\n",
    "    except Exception as e:\n",
    "        print(f\"\\n--- ERROR ---\\nFailed to initialize Binoculars classifier: {e}\")\n",
    "        return\n",
    "\n",
    "    text_samples = df[\"text\"].tolist()\n",
    "    y_true = df[\"label\"].astype(int).tolist()\n",
    "    y_pred, all_scores = [], []\n",
    "\n",
    "    print(f\"\\nRunning predictions on {len(text_samples)} samples in batches of {batch_size}...\")\n",
    "    for i in tqdm(range(0, len(text_samples), batch_size), desc=\"Processing Batches\"):\n",
    "        batch_texts = text_samples[i:i + batch_size]\n",
    "        batch_scores = binoculars.compute_score(batch_texts)\n",
    "        batch_preds = (np.array(batch_scores) < binoculars.threshold).astype(int).tolist()  # 1=ai, 0=human\n",
    "        y_pred.extend(batch_preds)\n",
    "        all_scores.extend(batch_scores)\n",
    "\n",
    "    results_df = pd.DataFrame({\n",
    "        'true_label': y_true,\n",
    "        'predicted_label': y_pred,\n",
    "        'binoculars_score': all_scores\n",
    "    })\n",
    "\n",
    "    y_true_arr = np.array(y_true, dtype=int)\n",
    "    y_pred_arr = np.array(y_pred, dtype=int)\n",
    "    y_scores = -np.array(all_scores, dtype=float)  # higher => more likely ai\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"      Binoculars Classification Statistics\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    print(\"\\n--- Classification Report (labels 0/1) ---\")\n",
    "    try:\n",
    "        print(classification_report(y_true_arr, y_pred_arr, labels=[0, 1], digits=4))\n",
    "    except Exception as e:\n",
    "        print(f\"Could not print classification report: {e}\")\n",
    "\n",
    "    accuracy = accuracy_score(y_true_arr, y_pred_arr)\n",
    "    print(f\"\\nOverall Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    try:\n",
    "        auc_score = roc_auc_score(y_true_arr, y_scores)\n",
    "        print(f\"ROC AUC Score: {auc_score:.4f}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Could not calculate ROC AUC Score: {e}\")\n",
    "\n",
    "    try:\n",
    "        cm = confusion_matrix(y_true_arr, y_pred_arr, labels=[0, 1])\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        fpr = fp / (fp + tn) if (fp + tn) > 0 else 0.0\n",
    "\n",
    "        print(\"\\n--- Confusion Matrix (rows=true, cols=pred) ---\")\n",
    "        print(cm)\n",
    "        print(f\"\\nFalse Positive Rate (human misclassified as ai): {fpr:.4f}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Could not calculate Confusion Matrix or FPR: {e}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    return results_df\n",
    "\n",
    "# --- MAIN EXECUTION (example) ---\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Loading and preparing the dataset (assumes raid_eval, m4_eval, cheat_eval exist)...\")\n",
    "    # You can increase batch_size as your memory+seq length allows (try 32 -> 64 -> 128)\n",
    "    evaluate_on_dataframe(raid_eval, batch_size=32)\n",
    "    evaluate_on_dataframe(m4_eval, batch_size=32)\n",
    "    evaluate_on_dataframe(cheat_eval, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jivnesh/anaconda3/envs/harshitml/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Initializing Fast-DetectGPT Detector ---\n",
      "Loading model google/gemma-3-4b-it...\n",
      "-> Loading model in bfloat16 (half-precision)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "2026-01-31 05:49:28.722833: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2026-01-31 05:49:28.753859: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1769806168.788149 1610340 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1769806168.797663 1610340 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1769806168.821830 1610340 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1769806168.821843 1610340 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1769806168.821845 1610340 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1769806168.821847 1610340 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2026-01-31 05:49:28.827565: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Skipping import of cpp extensions due to incompatible torch version 2.8.0+cu128 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [01:21<00:00, 40.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running detection on 10000 samples ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing samples: 100%|██████████| 10000/10000 [13:09<00:00, 12.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "      Fast-DetectGPT Classification Statistics\n",
      "==================================================\n",
      "\n",
      "--- Classification Report ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Human       0.52      1.00      0.69      5000\n",
      "          AI       0.97      0.09      0.17      5000\n",
      "\n",
      "    accuracy                           0.54     10000\n",
      "   macro avg       0.75      0.54      0.43     10000\n",
      "weighted avg       0.75      0.54      0.43     10000\n",
      "\n",
      "\n",
      "Overall Accuracy: 0.5436\n",
      "ROC AUC Score: 0.8214\n",
      "\n",
      "--- Confusion Matrix ---\n",
      "                | Predicted Human | Predicted AI   \n",
      "--------------------------------------------------\n",
      "Actual Human    | 4985            | 15             \n",
      "Actual AI       | 4549            | 451            \n",
      "--------------------------------------------------\n",
      "\n",
      "False Positive Rate (FPR): 0.0030 (Human text incorrectly flagged as AI)\n",
      "\n",
      "==================================================\n",
      "\n",
      "--- Running detection on 10000 samples ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing samples: 100%|██████████| 10000/10000 [15:09<00:00, 11.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "      Fast-DetectGPT Classification Statistics\n",
      "==================================================\n",
      "\n",
      "--- Classification Report ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Human       0.50      1.00      0.67      5000\n",
      "          AI       0.00      0.00      0.00      5000\n",
      "\n",
      "    accuracy                           0.50     10000\n",
      "   macro avg       0.25      0.50      0.33     10000\n",
      "weighted avg       0.25      0.50      0.33     10000\n",
      "\n",
      "\n",
      "Overall Accuracy: 0.4999\n",
      "ROC AUC Score: 0.4958\n",
      "\n",
      "--- Confusion Matrix ---\n",
      "                | Predicted Human | Predicted AI   \n",
      "--------------------------------------------------\n",
      "Actual Human    | 4999            | 1              \n",
      "Actual AI       | 5000            | 0              \n",
      "--------------------------------------------------\n",
      "\n",
      "False Positive Rate (FPR): 0.0002 (Human text incorrectly flagged as AI)\n",
      "\n",
      "==================================================\n",
      "\n",
      "--- Running detection on 12000 samples ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing samples: 100%|██████████| 12000/12000 [15:58<00:00, 12.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "      Fast-DetectGPT Classification Statistics\n",
      "==================================================\n",
      "\n",
      "--- Classification Report ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Human       0.34      1.00      0.50      4000\n",
      "          AI       0.92      0.02      0.04      8000\n",
      "\n",
      "    accuracy                           0.35     12000\n",
      "   macro avg       0.63      0.51      0.27     12000\n",
      "weighted avg       0.73      0.35      0.20     12000\n",
      "\n",
      "\n",
      "Overall Accuracy: 0.3464\n",
      "ROC AUC Score: 0.5795\n",
      "\n",
      "--- Confusion Matrix ---\n",
      "                | Predicted Human | Predicted AI   \n",
      "--------------------------------------------------\n",
      "Actual Human    | 3986            | 14             \n",
      "Actual AI       | 7829            | 171            \n",
      "--------------------------------------------------\n",
      "\n",
      "False Positive Rate (FPR): 0.0035 (Human text incorrectly flagged as AI)\n",
      "\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# A dictionary to map short names to Hugging Face model identifiers\n",
    "model_fullnames = {\n",
    "    'gemma3-4b': 'google/gemma-3-4b-it', # Use instruct-tuned version for better performance\n",
    "}\n",
    "\n",
    "def get_model_fullname(model_name):\n",
    "    return model_fullnames.get(model_name, model_name)\n",
    "\n",
    "def load_model(model_name, device, cache_dir, quantization=None):\n",
    "    model_fullname = get_model_fullname(model_name)\n",
    "    print(f'Loading model {model_fullname}...')\n",
    "    model_kwargs = {\"cache_dir\": cache_dir}\n",
    "    print(\"-> Loading model in bfloat16 (half-precision)...\")\n",
    "    model_kwargs[\"torch_dtype\"] = torch.bfloat16\n",
    "    model_kwargs[\"device_map\"] = \"auto\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_fullname, **model_kwargs)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def load_tokenizer(model_name, cache_dir):\n",
    "    model_fullname = get_model_fullname(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_fullname, cache_dir=cache_dir)\n",
    "    if tokenizer.pad_token_id is None:\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    return tokenizer\n",
    "\n",
    "def get_sampling_discrepancy_analytic(logits_ref, logits_score, labels):\n",
    "    if logits_ref.size(-1) != logits_score.size(-1):\n",
    "        vocab_size = min(logits_ref.size(-1), logits_score.size(-1))\n",
    "        logits_ref = logits_ref[:, :, :vocab_size]\n",
    "        logits_score = logits_score[:, :, :vocab_size]\n",
    "    labels = labels.unsqueeze(-1) if labels.ndim == logits_score.ndim - 1 else labels\n",
    "    lprobs_score = torch.log_softmax(logits_score, dim=-1)\n",
    "    probs_ref = torch.softmax(logits_ref, dim=-1)\n",
    "    log_likelihood = lprobs_score.gather(dim=-1, index=labels).squeeze(-1)\n",
    "    mean_ref = (probs_ref * lprobs_score).sum(dim=-1)\n",
    "    var_ref = (probs_ref * torch.square(lprobs_score)).sum(dim=-1) - torch.square(mean_ref)\n",
    "    log_likelihood_sum = log_likelihood.sum(dim=-1)\n",
    "    mean_ref_sum = mean_ref.sum(dim=-1)\n",
    "    var_ref_sum = var_ref.sum(dim=-1)\n",
    "    denominator = torch.sqrt(torch.relu(var_ref_sum)) + 1e-6\n",
    "    discrepancy = (log_likelihood_sum - mean_ref_sum) / denominator\n",
    "    return discrepancy.item()\n",
    "\n",
    "def compute_prob_norm(x, mu0, sigma0, mu1, sigma1):\n",
    "    pdf_value0 = norm.pdf(x, loc=mu0, scale=sigma0)\n",
    "    pdf_value1 = norm.pdf(x, loc=mu1, scale=sigma1)\n",
    "    prob = pdf_value1 / (pdf_value0 + pdf_value1 + 1e-6)\n",
    "    return prob\n",
    "\n",
    "class FastDetectGPTDetector:\n",
    "    def __init__(self, scoring_model_name, sampling_model_name, device, cache_dir, quantization):\n",
    "        self.scoring_model_name = scoring_model_name\n",
    "        self.sampling_model_name = sampling_model_name\n",
    "        self.scoring_tokenizer = load_tokenizer(scoring_model_name, cache_dir)\n",
    "        self.scoring_model = load_model(scoring_model_name, device, cache_dir, quantization)\n",
    "        if sampling_model_name == scoring_model_name:\n",
    "            self.sampling_model = self.scoring_model\n",
    "            self.sampling_tokenizer = self.scoring_tokenizer\n",
    "        else:\n",
    "            self.sampling_tokenizer = load_tokenizer(sampling_model_name, cache_dir)\n",
    "            self.sampling_model = load_model(sampling_model_name, device, cache_dir, quantization)\n",
    "        # Using pre-calibrated parameters\n",
    "        self.classifier_params = {'mu0': -0.0707, 'sigma0': 0.9520, 'mu1': 2.9306, 'sigma1': 1.9039}\n",
    "\n",
    "    def compute_prob(self, text):\n",
    "        tokenized_score = self.scoring_tokenizer(text, truncation=True, return_tensors=\"pt\", max_length=1024)\n",
    "        labels = tokenized_score.input_ids[:, 1:].to(self.scoring_model.device)\n",
    "        if labels.shape[1] == 0:\n",
    "            return 0.0\n",
    "        with torch.no_grad():\n",
    "            inputs_score = {k: v.to(self.scoring_model.device) for k, v in tokenized_score.items()}\n",
    "            logits_score = self.scoring_model(**inputs_score).logits[:, :-1]\n",
    "            if self.sampling_model_name == self.scoring_model_name:\n",
    "                logits_ref = logits_score\n",
    "            else:\n",
    "                tokenized_ref = self.sampling_tokenizer(text, truncation=True, return_tensors=\"pt\", max_length=1024)\n",
    "                inputs_ref = {k: v.to(self.sampling_model.device) for k, v in tokenized_ref.items()}\n",
    "                logits_ref = self.sampling_model(**inputs_ref).logits[:, :-1]\n",
    "        crit = get_sampling_discrepancy_analytic(logits_ref, logits_score, labels)\n",
    "        prob = compute_prob_norm(crit, **self.classifier_params)\n",
    "        return prob\n",
    "\n",
    "# --- Script Configuration ---\n",
    "SCORING_MODEL_NAME = \"gemma3-4b\"\n",
    "SAMPLING_MODEL_NAME = \"gemma3-4b\" # Use the same model for simplicity\n",
    "DEVICE = \"cuda:03\" if torch.cuda.is_available() else \"cpu\"\n",
    "CACHE_DIR = \"./model_cache\"\n",
    "OUTPUT_FILE = \"fastdetectgpt_gemma_results.csv\"\n",
    "\n",
    "# --- Main Execution ---\n",
    "print(\"--- Initializing Fast-DetectGPT Detector ---\")\n",
    "detector = FastDetectGPTDetector(\n",
    "    scoring_model_name=SCORING_MODEL_NAME,\n",
    "    sampling_model_name=SAMPLING_MODEL_NAME,\n",
    "    device=DEVICE,\n",
    "    cache_dir=CACHE_DIR,\n",
    "    quantization=None\n",
    ")\n",
    "\n",
    "\n",
    "def evaluate_detector(df, detector):\n",
    "    \"\"\"\n",
    "    Runs the Fast-DetectGPT detector on a given DataFrame and evaluates the results.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame with 'text' and 'label' columns. \n",
    "                           'label' should be 0 for human and 1 for AI.\n",
    "        detector (FastDetectGPTDetector): An initialized detector instance.\n",
    "        output_filename (str): The path to save the detailed results CSV file.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Running detection on {len(df)} samples ---\")\n",
    "    all_probs = []\n",
    "    true_labels = []\n",
    "\n",
    "    # Ensure the DataFrame has the required columns\n",
    "    if 'text' not in df.columns or 'label' not in df.columns:\n",
    "        raise ValueError(\"Input DataFrame must contain 'text' and 'label' columns.\")\n",
    "\n",
    "    for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing samples\"):\n",
    "        try:\n",
    "            prob = detector.compute_prob(row['text'])\n",
    "            all_probs.append(prob)\n",
    "            true_labels.append(row['label'])\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing sample (index {index}): {e}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "    # --- Evaluating Results ---\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"      Fast-DetectGPT Classification Statistics\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    if len(all_probs) > 0 and len(set(true_labels)) > 1:\n",
    "        # Convert probabilities to binary predictions for classification report\n",
    "        binary_predictions = [1 if p > 0.5 else 0 for p in all_probs]\n",
    "\n",
    "        # 1. Classification Report (Precision, Recall, F1-Score)\n",
    "        print(\"\\n--- Classification Report ---\")\n",
    "        print(classification_report(true_labels, binary_predictions, target_names=['Human', 'AI']))\n",
    "\n",
    "        # 2. Overall Accuracy\n",
    "        accuracy = accuracy_score(true_labels, binary_predictions)\n",
    "        print(f\"\\nOverall Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "        # 3. ROC AUC Score\n",
    "        roc_auc = roc_auc_score(true_labels, all_probs)\n",
    "        print(f\"ROC AUC Score: {roc_auc:.4f}\")\n",
    "\n",
    "        # 4. Confusion Matrix and FPR\n",
    "        cm = confusion_matrix(true_labels, binary_predictions)\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        fpr = fp / (fp + tn) if (fp + tn) > 0 else 0.0\n",
    "\n",
    "        print(\"\\n--- Confusion Matrix ---\")\n",
    "        print(f\"{'':<15} | {'Predicted Human':<15} | {'Predicted AI':<15}\")\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"{'Actual Human':<15} | {tn:<15} | {fp:<15}\")\n",
    "        print(f\"{'Actual AI':<15} | {fn:<15} | {tp:<15}\")\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"\\nFalse Positive Rate (FPR): {fpr:.4f} (Human text incorrectly flagged as AI)\")\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "        # results_df = pd.DataFrame({'text': df['text'], 'true_label': true_labels, 'predicted_prob_ai': all_probs})\n",
    "        # output_filename=\"fastdetectgpt_gemma_results.csv\"\n",
    "        # results_df.to_csv(output_filename, index=False)\n",
    "        # print(f\"Detailed results saved to {output_filename}\")\n",
    "    else:\n",
    "        print(f\"Could not compute metrics. Processed {len(all_probs)} samples.\")\n",
    "\n",
    "# Example usage with one of the provided dataframes (e.g., cheat_eval)\n",
    "# You can replace `cheat_eval` with `m4_eval` or `raid_eval`\n",
    "# Note: `raid_eval` has a 'models' column, but the function uses the 'label' column as requested.\n",
    "evaluate_detector(cheat_eval, detector)\n",
    "evaluate_detector(m4_eval, detector)\n",
    "evaluate_detector(raid_eval, detector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "harshitml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

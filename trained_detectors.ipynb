{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d78067a8",
   "metadata": {},
   "source": [
    "# BERT Model Testing and Text Modification Experiments\n",
    "\n",
    "This notebook loads the pre-trained BERT model and saved test data from `bert_finetuning_pipeline.ipynb` to conduct text modification experiments and accuracy comparisons.\n",
    "\n",
    "## Workflow:\n",
    "1. Load saved model and test data\n",
    "2. Establish baseline accuracy\n",
    "3. Apply text modifications to test data\n",
    "4. Compare modified test accuracies with baseline\n",
    "5. Analyze results and impacts\n",
    "\n",
    "**Prerequisites**: Run `bert_finetuning_pipeline.ipynb` first to train and save the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a0893d",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb141d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nltk\n",
    "# Cell for installing new dependencies\n",
    "# !pip install tensorflow tensorflow-hub nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6afca30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded:\n",
      "  Dataset base path: /home/jivnesh/Harshit_Surge/dataset/eval_dataset\n",
      "  Model checkpoint: best_model.pt\n",
      "  Simple BERT checkpoint: best_simplebert.pt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# ====================================\n",
    "# CONFIGURATION - Path Variables\n",
    "# ====================================\n",
    "# Update these paths to match your environment\n",
    "\n",
    "# Base directory for datasets\n",
    "DATASET_BASE_PATH = \"/home/jivnesh/Harshit_Surge/dataset/eval_dataset\"\n",
    "\n",
    "# Individual dataset paths\n",
    "RAID_EVAL_PATH = os.path.join(DATASET_BASE_PATH, \"raid_eval.csv\")\n",
    "M4_EVAL_PATH = os.path.join(DATASET_BASE_PATH, \"m4_eval.csv\")\n",
    "CHEAT_EVAL_PATH = os.path.join(DATASET_BASE_PATH, \"cheat_eval.csv\")\n",
    "HC3_EVAL_PATH = os.path.join(DATASET_BASE_PATH, \"hc3_eval.csv\")\n",
    "MAGE_EVAL_PATH = os.path.join(DATASET_BASE_PATH, \"mage_eval.csv\")\n",
    "\n",
    "# Model checkpoint paths\n",
    "BEST_MODEL_CHECKPOINT = \"best_model.pt\"\n",
    "BEST_SIMPLEBERT_CHECKPOINT = \"best_simplebert.pt\"\n",
    "\n",
    "# Optional: Create a dictionary for easy dataset access\n",
    "DATASET_PATHS = {\n",
    "    'raid_eval': RAID_EVAL_PATH,\n",
    "    'm4_eval': M4_EVAL_PATH,\n",
    "    'cheat_eval': CHEAT_EVAL_PATH,\n",
    "    'hc3_eval': HC3_EVAL_PATH,\n",
    "    'mage_eval': MAGE_EVAL_PATH,\n",
    "}\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "print(f\"  Dataset base path: {DATASET_BASE_PATH}\")\n",
    "print(f\"  Model checkpoint: {BEST_MODEL_CHECKPOINT}\")\n",
    "print(f\"  Simple BERT checkpoint: {BEST_SIMPLEBERT_CHECKPOINT}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5eacba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jivnesh/anaconda3/envs/harshitml/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch 2.8.0+cu128\n",
      "CUDA available: True\n",
      "GPU: NVIDIA A100 80GB PCIe\n",
      "Using device: cuda:1\n"
     ]
    }
   ],
   "source": [
    "# Cell 1 - Imports and setup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    AutoConfig\n",
    ")\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "seed_val = 42\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "print(f\"Using PyTorch {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b717ac9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------\n",
    "#  Transformer parameters\n",
    "#--------------------------------\n",
    "max_seq_length = 64\n",
    "batch_size = 128\n",
    "\n",
    "#--------------------------------\n",
    "#  GAN-BERT specific parameters\n",
    "#--------------------------------\n",
    "# number of hidden layers in the generator, \n",
    "# each of the size of the output space\n",
    "num_hidden_layers_g = 2; \n",
    "# number of hidden layers in the discriminator, \n",
    "# each of the size of the input space\n",
    "num_hidden_layers_d = 2; \n",
    "# size of the generator's input noisy vectors\n",
    "noise_size = 150\n",
    "# dropout to be applied to discriminator's input vectors\n",
    "out_dropout_rate = 0.2\n",
    "\n",
    "# Replicate labeled data to balance poorly represented datasets, \n",
    "# e.g., less than 1% of labeled material\n",
    "apply_balance = True\n",
    "\n",
    "#--------------------------------\n",
    "#  Optimization parameters\n",
    "#--------------------------------\n",
    "learning_rate_discriminator = 5e-5\n",
    "learning_rate_generator = 5e-5\n",
    "epsilon = 1e-8\n",
    "num_train_epochs = 10\n",
    "multi_gpu = True\n",
    "# Scheduler\n",
    "apply_scheduler = False\n",
    "warmup_proportion = 0.1\n",
    "# Print\n",
    "print_each_n_step = 10\n",
    "label_list = [\"human\", \"ai\"]\n",
    "#--------------------------------\n",
    "#  Adopted Tranformer model\n",
    "#--------------------------------\n",
    "# Since this version is compatible with Huggingface transformers, you can uncomment\n",
    "# (or add) transformer models compatible with GAN\n",
    "\n",
    "# model_name = \"bert-base-cased\"\n",
    "#model_name = \"bert-base-uncased\"\n",
    "#model_name = \"roberta-base\"\n",
    "#model_name = \"albert-base-v2\"\n",
    "#model_name = \"xlm-roberta-base\"\n",
    "#model_name = \"amazon/bort\"\n",
    "\n",
    "#--------------------------------\n",
    "#  Retrieve the TREC QC Dataset\n",
    "#--------------------------------\n",
    "# ! git clone https://github.com/crux82/ganbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d118e105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory cleared successfully!\n",
      "GPU memory allocated: 0.00 MB\n",
      "GPU memory cached: 0.00 MB\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "# Clear GPU memory to prevent out-of-memory issues\n",
    "\n",
    "# Clear PyTorch cache\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "# Force garbage collection\n",
    "gc.collect()\n",
    "\n",
    "print(\"GPU memory cleared successfully!\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU memory allocated: {torch.cuda.memory_allocated(device) / 1024**2:.2f} MB\")\n",
    "    print(f\"GPU memory cached: {torch.cuda.memory_reserved(device) / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae35d4c",
   "metadata": {},
   "source": [
    "## Load Trained GANBERT From Checkpoint (Evaluation Only)\n",
    "The following cells reconstruct the trained transformer + generator + discriminator from a saved checkpoint (e.g. `best_model.pt`) and provide a helper to run evaluation using existing `evaluate_on_dataloader` or `evaluate` functions. Adjust `checkpoint_path` as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f14fe76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------\n",
    "#   The Generator as in \n",
    "#   https://www.aclweb.org/anthology/2020.acl-main.191/\n",
    "#   https://github.com/crux82/ganbert\n",
    "#------------------------------\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, noise_size=100, output_size=512, hidden_sizes=[512], dropout_rate=0.1):\n",
    "        super(Generator, self).__init__()\n",
    "        layers = []\n",
    "        hidden_sizes = [noise_size] + hidden_sizes\n",
    "        for i in range(len(hidden_sizes)-1):\n",
    "            layers.extend([nn.Linear(hidden_sizes[i], hidden_sizes[i+1]), nn.LeakyReLU(0.2, inplace=True), nn.Dropout(dropout_rate)])\n",
    "\n",
    "        layers.append(nn.Linear(hidden_sizes[-1],output_size))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, noise):\n",
    "        output_rep = self.layers(noise)\n",
    "        return output_rep\n",
    "\n",
    "#------------------------------\n",
    "#   The Discriminator\n",
    "#   https://www.aclweb.org/anthology/2020.acl-main.191/\n",
    "#   https://github.com/crux82/ganbert\n",
    "#------------------------------\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_size=512, hidden_sizes=[512], num_labels=2, dropout_rate=0.1):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.input_dropout = nn.Dropout(p=dropout_rate)\n",
    "        layers = []\n",
    "        hidden_sizes = [input_size] + hidden_sizes\n",
    "        for i in range(len(hidden_sizes)-1):\n",
    "            layers.extend([nn.Linear(hidden_sizes[i], hidden_sizes[i+1]), nn.LeakyReLU(0.2, inplace=True), nn.Dropout(dropout_rate)])\n",
    "\n",
    "        self.layers = nn.Sequential(*layers) #per il flatten\n",
    "        self.logit = nn.Linear(hidden_sizes[-1],num_labels+1) # +1 for the probability of this sample being fake/real.\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, input_rep):\n",
    "        input_rep = self.input_dropout(input_rep)\n",
    "        last_rep = self.layers(input_rep)\n",
    "        logits = self.logit(last_rep)\n",
    "        probs = self.softmax(logits)\n",
    "        return last_rep, logits, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8194e0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reusable evaluation function for any dataloader\n",
    "from typing import Dict, Any\n",
    "\n",
    "def evaluate_on_dataloader(eval_name: str, eval_dataloader) -> Dict[str, Any]:\n",
    "    transformer.eval()\n",
    "    discriminator.eval()\n",
    "    generator.eval()\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels_ids = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in eval_dataloader:\n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].to(device)\n",
    "\n",
    "            outputs = transformer(\n",
    "                b_input_ids,\n",
    "                attention_mask=b_input_mask,\n",
    "                return_dict=True\n",
    "            )\n",
    "            last_hidden_state = outputs.last_hidden_state\n",
    "            cls_embed = last_hidden_state[:, 0, :]\n",
    "            mask_exp = b_input_mask.unsqueeze(-1).float()\n",
    "            mean_embed = (last_hidden_state * mask_exp).sum(1) / mask_exp.sum(1)\n",
    "            sent_rep = torch.cat([cls_embed, mean_embed], dim=1)\n",
    "\n",
    "            _, logits, probs = discriminator(sent_rep)\n",
    "            filtered_logits = logits[:, 0:-1]\n",
    "            _, preds = torch.max(filtered_logits, 1)\n",
    "\n",
    "            all_preds += preds.detach().cpu()\n",
    "            all_labels_ids += b_labels.detach().cpu()\n",
    "\n",
    "    all_preds = torch.stack(all_preds).numpy()\n",
    "    all_labels_ids = torch.stack(all_labels_ids).numpy()\n",
    "\n",
    "    acc = np.sum(all_preds == all_labels_ids) / len(all_preds)\n",
    "    report = classification_report(all_labels_ids, all_preds, target_names=label_list, zero_division=0, output_dict=True)\n",
    "    cm = confusion_matrix(all_labels_ids, all_preds)\n",
    "\n",
    "    print(f\"\\n[{eval_name}] Accuracy: {acc:.3f}\")\n",
    "    print(f\"[{eval_name}] Classification Report:\")\n",
    "    from pprint import pprint\n",
    "    pprint(report)\n",
    "    print(f\"[{eval_name}] Confusion Matrix:\\n{cm}\")\n",
    "\n",
    "    # Also print FPR explicitly\n",
    "    fpr = np.sum((all_preds == 1) & (all_labels_ids == 0)) / max(1, np.sum(all_labels_ids == 0))\n",
    "    print(f\"[{eval_name}] False Positive Rate: {fpr:.3f}\")\n",
    "\n",
    "    return {\n",
    "        'name': eval_name,\n",
    "        'accuracy': float(acc),\n",
    "        'classification_report': report,\n",
    "        'confusion_matrix': cm.tolist(),\n",
    "        'false_positive_rate': float(fpr),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4d62c5",
   "metadata": {},
   "source": [
    "## Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98013393",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_loader(df,batch_size=32, do_shuffle=False, balance_label_examples=False):\n",
    "    \"\"\"\n",
    "    Build a DataLoader from a DataFrame with columns:\n",
    "      - text (str)\n",
    "      - label (int: 0=human, 1=ai)\n",
    "\n",
    "    Returns batches of:\n",
    "      (input_ids, attention_mask, label_ids, label_mask)\n",
    "\n",
    "    label_mask is all ones (fully labeled dataset).\n",
    "    \"\"\"\n",
    "    # Optional class balancing (simple minority oversampling)\n",
    "    if balance_label_examples:\n",
    "        class_counts = df['label'].value_counts()\n",
    "        if len(class_counts) == 2:\n",
    "            max_count = class_counts.max()\n",
    "            dfs = []\n",
    "            for lbl, cnt in class_counts.items():\n",
    "                sub = df[df.label == lbl]\n",
    "                if cnt < max_count:\n",
    "                    reps = max_count - cnt\n",
    "                    # oversample with replacement\n",
    "                    extra = sub.sample(reps, replace=True, random_state=seed_val)\n",
    "                    sub = pd.concat([sub, extra], ignore_index=True)\n",
    "                dfs.append(sub)\n",
    "            df = pd.concat(dfs, ignore_index=True).sample(frac=1, random_state=seed_val).reset_index(drop=True)\n",
    "\n",
    "    texts = df['text'].tolist()\n",
    "    labels = df['label'].tolist()\n",
    "\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    for t in texts:\n",
    "        encoded = tokenizer.encode(\n",
    "            t,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_seq_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True\n",
    "        )\n",
    "        input_ids.append(encoded)\n",
    "        attention_masks.append([int(tok_id > 0) for tok_id in encoded])\n",
    "\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "    label_ids = torch.tensor(labels, dtype=torch.long)\n",
    "    label_mask_array = torch.ones(len(labels), dtype=torch.bool)\n",
    "\n",
    "    dataset = TensorDataset(input_ids, attention_masks, label_ids, label_mask_array)\n",
    "\n",
    "    sampler_cls = RandomSampler if do_shuffle else SequentialSampler\n",
    "    try:\n",
    "        bs = batch_size\n",
    "    except NameError:\n",
    "        bs = 32\n",
    "\n",
    "    return DataLoader(dataset, sampler=sampler_cls(dataset), batch_size=bs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b09d39d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1767299/3282586088.py:3: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  raid_eval[\"label\"]=raid_eval[\"models\"].replace({\"human\":0,\"ai\":1})\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "raid_eval=pd.read_csv(RAID_EVAL_PATH)\n",
    "raid_eval[\"label\"]=raid_eval[\"models\"].replace({\"human\":0,\"ai\":1})\n",
    "m4_eval=pd.read_csv(M4_EVAL_PATH)\n",
    "cheat_eval=pd.read_csv(CHEAT_EVAL_PATH)\n",
    "hc3_eval=pd.read_csv(HC3_EVAL_PATH)\n",
    "mage_eval=pd.read_csv(MAGE_EVAL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be3e0bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_on_df(eval_name: str, eval_df) -> Dict[str, Any]:\n",
    "    # create data loaders\n",
    "    print(f\"Evaluating on {eval_name} dataset with {len(eval_df)} samples.\")\n",
    "    dataloader=generate_data_loader(eval_df, batch_size=32, do_shuffle=False, balance_label_examples=False)\n",
    "    return evaluate_on_dataloader(eval_name, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "564f117f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint: best_model.pt -> device=cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 5, 'test_acc': 0.9595833333333333, 'config_keys': ['model_name', 'noise_size', 'hidden_size', 'hidden_levels_g', 'hidden_levels_d', 'num_hidden_layers_g', 'num_hidden_layers_d']}\n",
      "Loading base transformer: bert-base-cased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-31 15:18:02.780155: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2026-01-31 15:18:02.799719: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1769840282.819923 1767299 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1769840282.826529 1767299 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1769840282.843774 1767299 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1769840282.843790 1767299 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1769840282.843791 1767299 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1769840282.843793 1767299 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2026-01-31 15:18:02.849323: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Skipping import of cpp extensions due to incompatible torch version 2.8.0+cu128 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded architecture config: {'noise_size': 150, 'hidden_size': 1536, 'hidden_levels_g': [1536, 1536], 'hidden_levels_d': [1536, 1536], 'num_hidden_layers_g': 2, 'num_hidden_layers_d': 2}\n",
      "Generator kwargs: {'noise_size': 150, 'output_size': 1536, 'hidden_sizes': [1536, 1536]}\n",
      "Discriminator kwargs: {'input_size': 1536, 'hidden_sizes': [1536, 1536]}\n",
      "Transformer weights loaded.\n",
      "Generator weights loaded.\n",
      "Discriminator weights loaded.\n",
      "Checkpoint restoration complete.\n",
      "Label map: {'human': 0, 'ai': 1}\n",
      "Ready to evaluate: supply a DataLoader and call _call_evaluate().\n"
     ]
    }
   ],
   "source": [
    "# Load-and-evaluate snippet for GANBERT checkpoint\n",
    "import torch, os\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModel, AutoConfig\n",
    "\n",
    "# --- CONFIGURE ---\n",
    "checkpoint_path = BEST_MODEL_CHECKPOINT  # UPDATE to your actual saved file\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "map_location = None if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# --- Helper: DataParallel agnostic loader ---\n",
    "def load_state_into_model(model, state_dict):\n",
    "    \"\"\"Load state dict into model agnostic to DataParallel wrapping.\"\"\"\n",
    "    if not state_dict:  # empty dict\n",
    "        return model\n",
    "    keys = list(state_dict.keys())\n",
    "    if len(keys) == 0:\n",
    "        model.load_state_dict(state_dict)\n",
    "        return model\n",
    "    has_module_prefix = keys[0].startswith(\"module.\")\n",
    "    model_state_keys = list(model.state_dict().keys())\n",
    "    model_has_module = model_state_keys[0].startswith(\"module.\")\n",
    "    if has_module_prefix and not model_has_module:\n",
    "        fixed = {k.replace(\"module.\", \"\"): v for k, v in state_dict.items()}\n",
    "        model.load_state_dict(fixed)\n",
    "    elif (not has_module_prefix) and model_has_module:\n",
    "        fixed = {(\"module.\" + k): v for k, v in state_dict.items()}\n",
    "        model.load_state_dict(fixed)\n",
    "    else:\n",
    "        model.load_state_dict(state_dict)\n",
    "    return model\n",
    "\n",
    "if not os.path.exists(checkpoint_path):\n",
    "    raise FileNotFoundError(f\"Checkpoint not found: {checkpoint_path}\")\n",
    "print(f\"Loading checkpoint: {checkpoint_path} -> device={device}\")\n",
    "ckpt = torch.load(checkpoint_path, map_location=map_location)\n",
    "\n",
    "saved_epoch = ckpt.get('epoch')\n",
    "saved_test_acc = ckpt.get('test_accuracy')\n",
    "label_map = ckpt.get('label_map')\n",
    "saved_config = ckpt.get('config', {})\n",
    "model_name=saved_config.get('model_name')\n",
    "print({\"epoch\": saved_epoch, \"test_acc\": saved_test_acc, \"config_keys\": list(saved_config.keys())})\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)                                                                                          \n",
    "# --- Reconstruct transformer ---\n",
    "transformer = None\n",
    "model_name = saved_config.get('model_name')\n",
    "if model_name:\n",
    "    try:\n",
    "        print(f\"Loading base transformer: {model_name}\")\n",
    "        transformer = AutoModel.from_pretrained(model_name)\n",
    "    except Exception as e:\n",
    "        print(\"HF load failed; manually instantiate transformer.\", e)\n",
    "else:\n",
    "    print(\"No model_name in checkpoint config; provide transformer manually.\")\n",
    "\n",
    "# --- Reconstruct Generator / Discriminator (expect classes already defined in notebook) ---\n",
    "noise_size = saved_config.get('noise_size')\n",
    "hidden_size = saved_config.get('hidden_size')\n",
    "hidden_levels_g = saved_config.get('hidden_levels_g')\n",
    "hidden_levels_d = saved_config.get('hidden_levels_d')\n",
    "num_hidden_layers_g = saved_config.get('num_hidden_layers_g')\n",
    "num_hidden_layers_d = saved_config.get('num_hidden_layers_d')\n",
    "print(\"Loaded architecture config:\", {\n",
    "    'noise_size': noise_size,\n",
    "    'hidden_size': hidden_size,\n",
    "    'hidden_levels_g': hidden_levels_g,\n",
    "    'hidden_levels_d': hidden_levels_d,\n",
    "    'num_hidden_layers_g': num_hidden_layers_g,\n",
    "    'num_hidden_layers_d': num_hidden_layers_d,\n",
    "})\n",
    "\n",
    "try:\n",
    "    Generator; Discriminator\n",
    "except NameError:\n",
    "    raise RuntimeError(\"Define Generator and Discriminator classes (from training notebook) before running this cell.\")\n",
    "\n",
    "# Build kwargs adaptively (covers common naming patterns)\n",
    "gen_kwargs = {}\n",
    "disc_kwargs = {}\n",
    "if noise_size is not None: gen_kwargs['noise_size'] = noise_size\n",
    "if hidden_size is not None:\n",
    "    gen_kwargs['output_size'] = hidden_size  # training used output_size for generator final size\n",
    "    disc_kwargs['input_size'] = hidden_size\n",
    "if hidden_levels_g is not None: gen_kwargs['hidden_sizes'] = hidden_levels_g\n",
    "if hidden_levels_d is not None: disc_kwargs['hidden_sizes'] = hidden_levels_d\n",
    "if 'dropout_rate' in saved_config:  # pass through if recorded\n",
    "    gen_kwargs['dropout_rate'] = saved_config['dropout_rate']\n",
    "    disc_kwargs['dropout_rate'] = saved_config['dropout_rate']\n",
    "\n",
    "print(\"Generator kwargs:\", gen_kwargs)\n",
    "print(\"Discriminator kwargs:\", disc_kwargs)\n",
    "\n",
    "generator = Generator(**gen_kwargs)\n",
    "discriminator = Discriminator(num_labels=len(label_map) if label_map else 2, **disc_kwargs)\n",
    "\n",
    "# --- Load state dicts ---\n",
    "transformer_state = ckpt.get('transformer_state_dict') or ckpt.get('transformer')\n",
    "generator_state = ckpt.get('generator_state_dict')\n",
    "discriminator_state = ckpt.get('discriminator_state_dict') or ckpt.get('discriminator')\n",
    "\n",
    "if transformer is not None and transformer_state is not None:\n",
    "    load_state_into_model(transformer, transformer_state)\n",
    "    print('Transformer weights loaded.')\n",
    "else:\n",
    "    print('Transformer weights missing or transformer not instantiated.')\n",
    "\n",
    "if generator_state is not None:\n",
    "    load_state_into_model(generator, generator_state)\n",
    "    print('Generator weights loaded.')\n",
    "else:\n",
    "    print('No generator weights found.')\n",
    "\n",
    "if discriminator_state is not None:\n",
    "    load_state_into_model(discriminator, discriminator_state)\n",
    "    print('Discriminator weights loaded.')\n",
    "else:\n",
    "    print('No discriminator weights found.')\n",
    "\n",
    "# Move to device\n",
    "generator.to(device).eval()\n",
    "discriminator.to(device).eval()\n",
    "if transformer is not None: transformer.to(device).eval()\n",
    "\n",
    "print('Checkpoint restoration complete.')\n",
    "if label_map: print('Label map:', label_map)\n",
    "else: print('No label_map stored; using default order (0..N-1).')\n",
    "\n",
    "# --- Evaluation helper using existing evaluate_on_dataloader / evaluate ---\n",
    "def _call_evaluate(split_name, dataloader, **kwargs):\n",
    "    if 'evaluate_on_dataloader' in globals():\n",
    "        return evaluate_on_dataloader(split_name, dataloader, **kwargs)\n",
    "    elif 'evaluate' in globals():\n",
    "        return evaluate(split_name, dataloader, **kwargs)\n",
    "    else:\n",
    "        raise RuntimeError('Define evaluate_on_dataloader or evaluate before calling this.')\n",
    "\n",
    "# Example (uncomment after you prepare a DataLoader named new_test_dataloader):\n",
    "# results = _call_evaluate('New Dataset', new_test_dataloader)\n",
    "# print(results)\n",
    "\n",
    "print('Ready to evaluate: supply a DataLoader and call _call_evaluate().')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aeda66dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on hc3 dataset with 10000 samples.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[hc3] Accuracy: 0.832\n",
      "[hc3] Classification Report:\n",
      "{'accuracy': 0.8324,\n",
      " 'ai': {'f1-score': 0.8217779668226287,\n",
      "        'precision': 0.8773841961852861,\n",
      "        'recall': 0.7728,\n",
      "        'support': 5000.0},\n",
      " 'human': {'f1-score': 0.8418271045677614,\n",
      "           'precision': 0.7969978556111508,\n",
      "           'recall': 0.892,\n",
      "           'support': 5000.0},\n",
      " 'macro avg': {'f1-score': 0.831802535695195,\n",
      "               'precision': 0.8371910258982185,\n",
      "               'recall': 0.8324,\n",
      "               'support': 10000.0},\n",
      " 'weighted avg': {'f1-score': 0.8318025356951951,\n",
      "                  'precision': 0.8371910258982185,\n",
      "                  'recall': 0.8324,\n",
      "                  'support': 10000.0}}\n",
      "[hc3] Confusion Matrix:\n",
      "[[4460  540]\n",
      " [1136 3864]]\n",
      "[hc3] False Positive Rate: 0.108\n",
      "Evaluating on mage_eval dataset with 10000 samples.\n",
      "\n",
      "[mage_eval] Accuracy: 0.563\n",
      "[mage_eval] Classification Report:\n",
      "{'accuracy': 0.5628,\n",
      " 'ai': {'f1-score': 0.3895559899469422,\n",
      "        'precision': 0.6452358926919519,\n",
      "        'recall': 0.279,\n",
      "        'support': 5000.0},\n",
      " 'human': {'f1-score': 0.6594485122293192,\n",
      "           'precision': 0.5400612401122735,\n",
      "           'recall': 0.8466,\n",
      "           'support': 5000.0},\n",
      " 'macro avg': {'f1-score': 0.5245022510881308,\n",
      "               'precision': 0.5926485664021127,\n",
      "               'recall': 0.5628,\n",
      "               'support': 10000.0},\n",
      " 'weighted avg': {'f1-score': 0.5245022510881306,\n",
      "                  'precision': 0.5926485664021127,\n",
      "                  'recall': 0.5628,\n",
      "                  'support': 10000.0}}\n",
      "[mage_eval] Confusion Matrix:\n",
      "[[4233  767]\n",
      " [3605 1395]]\n",
      "[mage_eval] False Positive Rate: 0.153\n",
      "Evaluating on m4_eval dataset with 10000 samples.\n",
      "\n",
      "[m4_eval] Accuracy: 0.604\n",
      "[m4_eval] Classification Report:\n",
      "{'accuracy': 0.6037,\n",
      " 'ai': {'f1-score': 0.43571123451516447,\n",
      "        'precision': 0.7563025210084033,\n",
      "        'recall': 0.306,\n",
      "        'support': 5000.0},\n",
      " 'human': {'f1-score': 0.6946135470447715,\n",
      "           'precision': 0.5649993731979441,\n",
      "           'recall': 0.9014,\n",
      "           'support': 5000.0},\n",
      " 'macro avg': {'f1-score': 0.565162390779968,\n",
      "               'precision': 0.6606509471031737,\n",
      "               'recall': 0.6037,\n",
      "               'support': 10000.0},\n",
      " 'weighted avg': {'f1-score': 0.565162390779968,\n",
      "                  'precision': 0.6606509471031737,\n",
      "                  'recall': 0.6037,\n",
      "                  'support': 10000.0}}\n",
      "[m4_eval] Confusion Matrix:\n",
      "[[4507  493]\n",
      " [3470 1530]]\n",
      "[m4_eval] False Positive Rate: 0.099\n",
      "Evaluating on cheat_eval dataset with 10000 samples.\n",
      "\n",
      "[cheat_eval] Accuracy: 0.858\n",
      "[cheat_eval] Classification Report:\n",
      "{'accuracy': 0.8584,\n",
      " 'ai': {'f1-score': 0.851291745431632,\n",
      "        'precision': 0.8962848297213623,\n",
      "        'recall': 0.8106,\n",
      "        'support': 5000.0},\n",
      " 'human': {'f1-score': 0.864859706050773,\n",
      "           'precision': 0.8271266885724717,\n",
      "           'recall': 0.9062,\n",
      "           'support': 5000.0},\n",
      " 'macro avg': {'f1-score': 0.8580757257412025,\n",
      "               'precision': 0.861705759146917,\n",
      "               'recall': 0.8584,\n",
      "               'support': 10000.0},\n",
      " 'weighted avg': {'f1-score': 0.8580757257412026,\n",
      "                  'precision': 0.8617057591469169,\n",
      "                  'recall': 0.8584,\n",
      "                  'support': 10000.0}}\n",
      "[cheat_eval] Confusion Matrix:\n",
      "[[4531  469]\n",
      " [ 947 4053]]\n",
      "[cheat_eval] False Positive Rate: 0.094\n",
      "Evaluating on raid_eval dataset with 12000 samples.\n",
      "\n",
      "[raid_eval] Accuracy: 0.960\n",
      "[raid_eval] Classification Report:\n",
      "{'accuracy': 0.9595833333333333,\n",
      " 'ai': {'f1-score': 0.9697649772458076,\n",
      "        'precision': 0.9672926252953613,\n",
      "        'recall': 0.97225,\n",
      "        'support': 8000.0},\n",
      " 'human': {'f1-score': 0.939062696318633,\n",
      "           'precision': 0.9439252336448598,\n",
      "           'recall': 0.93425,\n",
      "           'support': 4000.0},\n",
      " 'macro avg': {'f1-score': 0.9544138367822204,\n",
      "               'precision': 0.9556089294701106,\n",
      "               'recall': 0.9532499999999999,\n",
      "               'support': 12000.0},\n",
      " 'weighted avg': {'f1-score': 0.9595308836034161,\n",
      "                  'precision': 0.959503494745194,\n",
      "                  'recall': 0.9595833333333333,\n",
      "                  'support': 12000.0}}\n",
      "[raid_eval] Confusion Matrix:\n",
      "[[3737  263]\n",
      " [ 222 7778]]\n",
      "[raid_eval] False Positive Rate: 0.066\n"
     ]
    }
   ],
   "source": [
    "results_pre = {\n",
    "    'hc3': evaluate_on_df('hc3', hc3_eval),\n",
    "    'mage_eval': evaluate_on_df('mage_eval', mage_eval),\n",
    "    'm4_eval': evaluate_on_df('m4_eval', m4_eval),\n",
    "    'cheat_eval': evaluate_on_df('cheat_eval', cheat_eval),\n",
    "    'raid_eval': evaluate_on_df('raid_eval', raid_eval),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ea6671d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc for hc3 is 0.8324\n",
      "acc for mage_eval is 0.5628\n",
      "acc for m4_eval is 0.6037\n",
      "acc for cheat_eval is 0.8584\n",
      "acc for raid_eval is 0.9595833333333333\n"
     ]
    }
   ],
   "source": [
    "for key in results_pre:\n",
    "    print(f\"acc for {key} is {results_pre[key]['accuracy']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "10e221b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Adapter] device -> cuda:1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Adapter] Checkpoint loaded. epoch=0, acc=0.0, model_name=bert-base-cased\n",
      "[Adapter] instantiating AutoModel.from_pretrained('bert-base-cased')\n",
      "[Adapter] full model state loaded.\n"
     ]
    }
   ],
   "source": [
    "# Safe loader + evaluator for your \"best_simplebert.pt\" checkpoints\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel\n",
    "import inspect\n",
    "\n",
    "class CheckpointAdapter:\n",
    "    \"\"\"\n",
    "    Loads checkpoints saved with your baseline training loop and provides\n",
    "    an isolated TransformerClassifier instance ready for evaluation on a single GPU.\n",
    "    \"\"\"\n",
    "    def __init__(self, device=None):\n",
    "        if device is None:\n",
    "            self.device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "        else:\n",
    "            self.device = torch.device(device)\n",
    "        print(f\"[Adapter] device -> {self.device}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def _fix_state_dict(state_dict):\n",
    "        if not state_dict:\n",
    "            return state_dict\n",
    "        keys = list(state_dict.keys())\n",
    "        if not keys:\n",
    "            return state_dict\n",
    "        has_module = keys[0].startswith(\"module.\")\n",
    "        if has_module:\n",
    "            return {k.replace(\"module.\", \"\"): v for k, v in state_dict.items()}\n",
    "        return state_dict\n",
    "\n",
    "    class TransformerClassifier(nn.Module):\n",
    "        def __init__(self, transformer_model, hidden_size, num_labels, dropout_rate=0.1):\n",
    "            super().__init__()\n",
    "            self.transformer = transformer_model\n",
    "            self.dropout = nn.Dropout(dropout_rate)\n",
    "            self.classifier = nn.Linear(hidden_size, num_labels)\n",
    "\n",
    "        def forward(self, input_ids, attention_mask=None):\n",
    "            outputs = self.transformer(input_ids, attention_mask=attention_mask)\n",
    "            hidden_states = outputs.last_hidden_state\n",
    "            if attention_mask is not None:\n",
    "                mask_expanded = attention_mask.unsqueeze(-1).expand_as(hidden_states).float()\n",
    "                sum_embeddings = torch.sum(hidden_states * mask_expanded, dim=1)\n",
    "                sum_mask = torch.clamp(mask_expanded.sum(dim=1), min=1e-9)\n",
    "                pooled = sum_embeddings / sum_mask\n",
    "            else:\n",
    "                pooled = hidden_states.mean(dim=1)\n",
    "            pooled = self.dropout(pooled)\n",
    "            return self.classifier(pooled)\n",
    "\n",
    "    def load_checkpoint(self, checkpoint_path, transformer_factory=None, strict_load=True):\n",
    "        if not os.path.exists(checkpoint_path):\n",
    "            raise FileNotFoundError(f\"Checkpoint not found: {checkpoint_path}\")\n",
    "        ckpt = torch.load(checkpoint_path, map_location=\"cpu\")\n",
    "        meta = {\n",
    "            'epoch': ckpt.get('epoch'),\n",
    "            'accuracy': ckpt.get('accuracy', ckpt.get('test_accuracy')),\n",
    "            'seed': ckpt.get('seed'),\n",
    "            'model_name': ckpt.get('model_name') or (ckpt.get('config', {}) or {}).get('model_name'),\n",
    "            'label_map': ckpt.get('label_map'),\n",
    "            'raw_ckpt': ckpt\n",
    "        }\n",
    "        print(f\"[Adapter] Checkpoint loaded. epoch={meta['epoch']}, acc={meta['accuracy']}, model_name={meta['model_name']}\")\n",
    "\n",
    "        # Figure out what was saved\n",
    "        full_classifier_state = ckpt.get('state_dict')\n",
    "        transformer_only_state = ckpt.get('transformer_state_dict')\n",
    "\n",
    "        # Instantiate transformer\n",
    "        if transformer_factory is not None:\n",
    "            try:\n",
    "                sig = inspect.signature(transformer_factory)\n",
    "                transformer = transformer_factory() if len(sig.parameters) == 0 else transformer_factory(meta['model_name'])\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(f\"transformer_factory failed: {e}\")\n",
    "        else:\n",
    "            if meta['model_name'] is None:\n",
    "                raise RuntimeError(\"No model_name in checkpoint and no transformer_factory provided.\")\n",
    "            print(f\"[Adapter] instantiating AutoModel.from_pretrained('{meta['model_name']}')\")\n",
    "            transformer = AutoModel.from_pretrained(meta['model_name'])\n",
    "\n",
    "        # Load transformer weights\n",
    "        if transformer_only_state is not None:\n",
    "            missing, unexpected = transformer.load_state_dict(transformer_only_state, strict=False)\n",
    "            if missing or unexpected:\n",
    "                print(f\"[Adapter] (info) transformer load missing={len(missing)} unexpected={len(unexpected)}\")\n",
    "\n",
    "        hidden_size = getattr(transformer.config, \"hidden_size\", None)\n",
    "        if hidden_size is None:\n",
    "            raise RuntimeError(\"Transformer config has no hidden_size.\")\n",
    "\n",
    "        # Infer num_labels\n",
    "        num_labels = None\n",
    "        if meta['label_map']:\n",
    "            try:\n",
    "                num_labels = len(meta['label_map'])\n",
    "            except Exception:\n",
    "                pass\n",
    "        if num_labels is None and full_classifier_state:\n",
    "            for k in ['classifier.weight', 'module.classifier.weight']:\n",
    "                if k in full_classifier_state:\n",
    "                    num_labels = full_classifier_state[k].shape[0]\n",
    "                    break\n",
    "        if num_labels is None:\n",
    "            raise RuntimeError(\"Could not infer num_labels (need label_map or classifier weights).\")\n",
    "\n",
    "        model = self.TransformerClassifier(transformer, hidden_size, num_labels).to(self.device)\n",
    "\n",
    "        # If a full model (transformer + classifier) state dict was saved, load it\n",
    "        if full_classifier_state:\n",
    "            fixed = self._fix_state_dict(full_classifier_state)\n",
    "            try:\n",
    "                model.load_state_dict(fixed, strict=strict_load)\n",
    "                print(\"[Adapter] full model state loaded.\")\n",
    "            except RuntimeError as e:\n",
    "                print(f\"[Adapter] strict load failed: {e}\")\n",
    "                if strict_load:\n",
    "                    print(\"[Adapter] retrying with strict=False\")\n",
    "                    model.load_state_dict(fixed, strict=False)\n",
    "\n",
    "        model.eval()\n",
    "        return model, meta\n",
    "\n",
    "    def evaluate_model(self, model, dataloader, eval_name=\"eval\", label_list=None):\n",
    "        import time, numpy as np\n",
    "        from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "        if label_list is None and 'label_list' in globals():\n",
    "            label_list = globals()['label_list']\n",
    "        model.to(self.device).eval()\n",
    "        all_preds, all_labels = [], []\n",
    "        t0 = time.time()\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                # Expect tuple: (input_ids, attention_mask, labels, *optional)\n",
    "                input_ids = batch[0].to(self.device)\n",
    "                attention_mask = batch[1].to(self.device) if len(batch) > 1 else None\n",
    "                labels = batch[2].to(self.device) if len(batch) > 2 else None\n",
    "                logits = model(input_ids, attention_mask=attention_mask)\n",
    "                preds = torch.argmax(logits, dim=-1)\n",
    "                if labels is not None:\n",
    "                    all_labels.extend(labels.cpu().tolist())\n",
    "                all_preds.extend(preds.cpu().tolist())\n",
    "        if all_labels:\n",
    "            acc = accuracy_score(all_labels, all_preds)\n",
    "            if label_list:\n",
    "                report = classification_report(all_labels, all_preds, target_names=label_list, zero_division=0, output_dict=True)\n",
    "            else:\n",
    "                report = classification_report(all_labels, all_preds, zero_division=0, output_dict=True)\n",
    "            cm = confusion_matrix(all_labels, all_preds).tolist()\n",
    "            # Assume positive class = 1 if binary\n",
    "            if len(set(all_labels)) == 2 and 1 in set(all_labels):\n",
    "                denom = max(1, sum(1 for x in all_labels if x == 0))\n",
    "                fpr = sum(1 for p, y in zip(all_preds, all_labels) if p == 1 and y == 0) / denom\n",
    "            else:\n",
    "                fpr = None\n",
    "        else:\n",
    "            acc, report, cm, fpr = None, {}, [], None\n",
    "        print(f\"[{eval_name}] done in {time.time()-t0:.1f}s acc={acc}\")\n",
    "        return {\n",
    "            \"name\": eval_name,\n",
    "            \"accuracy\": acc,\n",
    "            \"classification_report\": report,\n",
    "            \"confusion_matrix\": cm,\n",
    "            \"false_positive_rate\": fpr\n",
    "        }\n",
    "\n",
    "    def evaluate_on_df(self, model, eval_df, eval_name=\"df_eval\", batch_size=32):\n",
    "        if 'generate_data_loader' not in globals():\n",
    "            raise RuntimeError(\"generate_data_loader not defined in this notebook.\")\n",
    "        dataloader = generate_data_loader(eval_df, batch_size=batch_size, do_shuffle=False, balance_label_examples=False)\n",
    "        return self.evaluate_model(model, dataloader, eval_name=eval_name)\n",
    "\n",
    "# --------------------------\n",
    "# Example usage snippet\n",
    "# --------------------------\n",
    "# NOTE: change checkpoint_path to the path of your saved best_simplebert.pt\n",
    "checkpoint_path = BEST_SIMPLEBERT_CHECKPOINT  # <-- update if necessary\n",
    "adapter = CheckpointAdapter(device=None)  # will pick cuda:1 if available else cpu\n",
    "\n",
    "# If your training saved model_name in the checkpoint and you want the loader to instantiate HF model:\n",
    "model, metadata = adapter.load_checkpoint(checkpoint_path)\n",
    "\n",
    "# If you want to provide a custom transformer factory (for example, set local cache or different init):\n",
    "# model, metadata = adapter.load_checkpoint(checkpoint_path, transformer_factory=lambda name: AutoModel.from_pretrained(name, local_files_only=False))\n",
    "\n",
    "# Now evaluate:\n",
    "# Make sure you create new_test_dataloader in this notebook with the exact expected batch shape:\n",
    "# (input_ids, attention_mask, labels, label_mask)\n",
    "# Example: results = adapter.evaluate_model(model, new_test_dataloader)\n",
    "# It will call the existing `evaluate` or `evaluate_on_dataloader` function if present.\n",
    "#\n",
    "# Example call (uncomment when you have dataloader ready):\n",
    "# results = adapter.evaluate_model(model, new_test_dataloader)\n",
    "# print(\"Evaluation results:\", results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ef050bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hc3] done in 8.4s acc=0.8579\n",
      "[mage_eval] done in 8.3s acc=0.5725\n",
      "[m4_eval] done in 8.3s acc=0.6444\n",
      "[cheat_eval] done in 8.3s acc=0.8276\n",
      "[raid_eval] done in 10.0s acc=0.9414166666666667\n"
     ]
    }
   ],
   "source": [
    "# Re-evaluate datasets using the baseline adapter model instead of GANBERT discriminator\n",
    "results_bert = {\n",
    "    'hc3': adapter.evaluate_on_df(model, hc3_eval, eval_name='hc3'),\n",
    "    'mage_eval': adapter.evaluate_on_df(model, mage_eval, eval_name='mage_eval'),\n",
    "    'm4_eval': adapter.evaluate_on_df(model, m4_eval, eval_name='m4_eval'),\n",
    "    'cheat_eval': adapter.evaluate_on_df(model, cheat_eval, eval_name='cheat_eval'),\n",
    "    'raid_eval': adapter.evaluate_on_df(model, raid_eval, eval_name='raid_eval'),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c8507114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI probability: 2.3100389512364927e-07\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# --- Generator / Discriminator (same as training definition) ---\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, noise_size=100, output_size=512, hidden_sizes=[512], dropout_rate=0.1):\n",
    "        super(Generator, self).__init__()\n",
    "        layers = []\n",
    "        hidden_sizes = [noise_size] + hidden_sizes\n",
    "        for i in range(len(hidden_sizes)-1):\n",
    "            layers.extend([nn.Linear(hidden_sizes[i], hidden_sizes[i+1]),\n",
    "                           nn.LeakyReLU(0.2, inplace=True),\n",
    "                           nn.Dropout(dropout_rate)])\n",
    "        layers.append(nn.Linear(hidden_sizes[-1], output_size))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, noise):\n",
    "        return self.layers(noise)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_size=512, hidden_sizes=[512], num_labels=2, dropout_rate=0.1):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.input_dropout = nn.Dropout(p=dropout_rate)\n",
    "        layers = []\n",
    "        hidden_sizes = [input_size] + hidden_sizes\n",
    "        for i in range(len(hidden_sizes)-1):\n",
    "            layers.extend([nn.Linear(hidden_sizes[i], hidden_sizes[i+1]),\n",
    "                           nn.LeakyReLU(0.2, inplace=True),\n",
    "                           nn.Dropout(dropout_rate)])\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        self.logit = nn.Linear(hidden_sizes[-1], num_labels+1)  # +1 for fake class\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, input_rep):\n",
    "        input_rep = self.input_dropout(input_rep)\n",
    "        last_rep = self.layers(input_rep)\n",
    "        logits = self.logit(last_rep)\n",
    "        probs = self.softmax(logits)\n",
    "        return last_rep, logits, probs\n",
    "\n",
    "# --- Helper to load state dict agnostic to DataParallel/`module.` prefixes ---\n",
    "def load_state_into_model(model, state_dict):\n",
    "    if not state_dict:\n",
    "        return model\n",
    "    keys = list(state_dict.keys())\n",
    "    if len(keys) == 0:\n",
    "        model.load_state_dict(state_dict)\n",
    "        return model\n",
    "    has_module_prefix = keys[0].startswith(\"module.\")\n",
    "    model_state_keys = list(model.state_dict().keys())\n",
    "    model_has_module = model_state_keys[0].startswith(\"module.\")\n",
    "    if has_module_prefix and not model_has_module:\n",
    "        fixed = {k.replace(\"module.\", \"\"): v for k, v in state_dict.items()}\n",
    "        model.load_state_dict(fixed)\n",
    "    elif (not has_module_prefix) and model_has_module:\n",
    "        fixed = {(\"module.\" + k): v for k, v in state_dict.items()}\n",
    "        model.load_state_dict(fixed)\n",
    "    else:\n",
    "        model.load_state_dict(state_dict)\n",
    "    return model\n",
    "\n",
    "# --- Main wrapper class ---\n",
    "class GANBERTWrapper:\n",
    "    \"\"\"\n",
    "    Simple wrapper to load a GAN-BERT checkpoint and predict AI probability for single texts.\n",
    "    Usage:\n",
    "        w = GANBERTWrapper(\"best_model.pt\", device=\"cuda\" or \"cpu\", max_seq_length=64)\n",
    "        p = w.predict_proba(\"some text here\")  # float in [0,1] probability that text is 'ai'\n",
    "    \"\"\"\n",
    "    def __init__(self, checkpoint_path: str, device: str = None, max_seq_length: int = 64):\n",
    "        if device is None:\n",
    "            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.device = torch.device(device)\n",
    "        self.max_seq_length = max_seq_length\n",
    "\n",
    "        if not os.path.exists(checkpoint_path):\n",
    "            raise FileNotFoundError(f\"Checkpoint not found: {checkpoint_path}\")\n",
    "\n",
    "        # load checkpoint\n",
    "        ckpt = torch.load(checkpoint_path, map_location=self.device if self.device.type == \"cpu\" else None)\n",
    "\n",
    "        self.ckpt = ckpt  # keep for inspection if needed\n",
    "        saved_config = ckpt.get('config', {})\n",
    "        self.label_map = ckpt.get('label_map', None)  # may be dict or list\n",
    "        model_name = saved_config.get('model_name', None)\n",
    "\n",
    "        # tokenizer + transformer\n",
    "        if model_name is None:\n",
    "            raise RuntimeError(\"Checkpoint doesn't contain 'model_name' in config. \"\n",
    "                               \"Either add model_name to checkpoint config or modify wrapper to provide it.\")\n",
    "        self.model_name = model_name\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "\n",
    "        # instantiate transformer\n",
    "        try:\n",
    "            transformer = AutoModel.from_pretrained(self.model_name)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to instantiate transformer from '{self.model_name}': {e}\")\n",
    "        # load transformer weights if present\n",
    "        transformer_state = ckpt.get('transformer_state_dict') or ckpt.get('transformer')\n",
    "        if transformer_state is not None:\n",
    "            load_state_into_model(transformer, transformer_state)\n",
    "        transformer.to(self.device).eval()\n",
    "        self.transformer = transformer\n",
    "\n",
    "        # build generator/discriminator kwargs from saved_config\n",
    "        gen_kwargs = {}\n",
    "        disc_kwargs = {}\n",
    "        noise_size = saved_config.get('noise_size')\n",
    "        hidden_size = saved_config.get('hidden_size')\n",
    "        hidden_levels_g = saved_config.get('hidden_levels_g')\n",
    "        hidden_levels_d = saved_config.get('hidden_levels_d')\n",
    "        if noise_size is not None: gen_kwargs['noise_size'] = noise_size\n",
    "        if hidden_size is not None:\n",
    "            gen_kwargs['output_size'] = hidden_size\n",
    "            disc_kwargs['input_size'] = hidden_size\n",
    "        if hidden_levels_g is not None: gen_kwargs['hidden_sizes'] = hidden_levels_g\n",
    "        if hidden_levels_d is not None: disc_kwargs['hidden_sizes'] = hidden_levels_d\n",
    "        if 'dropout_rate' in saved_config:\n",
    "            gen_kwargs['dropout_rate'] = saved_config['dropout_rate']\n",
    "            disc_kwargs['dropout_rate'] = saved_config['dropout_rate']\n",
    "\n",
    "        # instantiate models\n",
    "        self.generator = Generator(**gen_kwargs)\n",
    "        self.discriminator = Discriminator(num_labels=(len(self.label_map) if self.label_map else 2), **disc_kwargs)\n",
    "\n",
    "        # load generator/discriminator weights if present\n",
    "        generator_state = ckpt.get('generator_state_dict')\n",
    "        discriminator_state = ckpt.get('discriminator_state_dict') or ckpt.get('discriminator')\n",
    "        if generator_state is not None:\n",
    "            load_state_into_model(self.generator, generator_state)\n",
    "        if discriminator_state is not None:\n",
    "            load_state_into_model(self.discriminator, discriminator_state)\n",
    "\n",
    "        # move to device and set eval mode\n",
    "        self.generator.to(self.device).eval()\n",
    "        self.discriminator.to(self.device).eval()\n",
    "\n",
    "        # figure ai label index\n",
    "        self.ai_label_index = self._resolve_ai_index()\n",
    "\n",
    "    def _resolve_ai_index(self):\n",
    "        \"\"\"\n",
    "        Resolve the index for the 'ai' label in the label map or default to 1.\n",
    "        Supports label_map being dict {'human':0,'ai':1} or list ['human','ai'].\n",
    "        \"\"\"\n",
    "        if self.label_map is None:\n",
    "            return 1\n",
    "        if isinstance(self.label_map, dict):\n",
    "            # label_map maps label->index or index->label? try both\n",
    "            if 'ai' in self.label_map:\n",
    "                return int(self.label_map['ai'])\n",
    "            # maybe it's index->label\n",
    "            for k, v in self.label_map.items():\n",
    "                if isinstance(k, (int, str)) and str(v).lower() == 'ai':\n",
    "                    try:\n",
    "                        return int(k)\n",
    "                    except:\n",
    "                        pass\n",
    "        if isinstance(self.label_map, (list, tuple)):\n",
    "            for i, v in enumerate(self.label_map):\n",
    "                if str(v).lower() == 'ai':\n",
    "                    return i\n",
    "        # fallback\n",
    "        return 1\n",
    "\n",
    "    def predict_proba(self, text: str) -> float:\n",
    "        \"\"\"\n",
    "        Predict probability that `text` is AI-generated.\n",
    "        Returns float in [0,1] representing P(ai).\n",
    "        \"\"\"\n",
    "        self.transformer.eval()\n",
    "        self.discriminator.eval()\n",
    "\n",
    "        encoded = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_seq_length,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        input_ids = encoded['input_ids'].to(self.device)\n",
    "        attention_mask = encoded['attention_mask'].to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.transformer(\n",
    "                input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                return_dict=True\n",
    "            )\n",
    "            last_hidden_state = outputs.last_hidden_state  # (1, seq_len, hidden)\n",
    "            cls_embed = last_hidden_state[:, 0, :]\n",
    "            mask_exp = attention_mask.unsqueeze(-1).float()\n",
    "            mean_embed = (last_hidden_state * mask_exp).sum(1) / mask_exp.sum(1)\n",
    "            sent_rep = torch.cat([cls_embed, mean_embed], dim=1)  # same concat used at training\n",
    "\n",
    "            _, logits, probs = self.discriminator(sent_rep)  # probs shape (1, num_labels+1)\n",
    "            # remove last \"fake/real\" column: model used last column for fake/real in training\n",
    "            probs_real = probs[:, :-1]  # shape (1, num_labels)\n",
    "            # ensure ai index within range\n",
    "            idx = int(self.ai_label_index)\n",
    "            if idx < 0 or idx >= probs_real.shape[1]:\n",
    "                # fallback: try to find 'ai' by ordering assumption: human=0 ai=1\n",
    "                idx = 1 if probs_real.shape[1] > 1 else 0\n",
    "            ai_prob = probs_real[0, idx].item()\n",
    "        return float(ai_prob)\n",
    "\n",
    "# --- Convenience function to evaluate many models for the same text ---\n",
    "def predict_from_many_models(checkpoint_paths, text, device=None, max_seq_length=64):\n",
    "    \"\"\"\n",
    "    checkpoint_paths: list of checkpoint file paths\n",
    "    text: string to evaluate\n",
    "    Returns: dict {checkpoint_path: probability}\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    for cp in checkpoint_paths:\n",
    "        w = GANBERTWrapper(cp, device=device, max_seq_length=max_seq_length)\n",
    "        results[cp] = w.predict_proba(text)\n",
    "    return results\n",
    "\n",
    "# -------------------------\n",
    "# Example usage:\n",
    "# -------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # single model\n",
    "    cp = \"best_model.pt\"   # replace with your path\n",
    "    w = GANBERTWrapper(cp, device=\"cuda:1\" if torch.cuda.is_available() else \"cpu\", max_seq_length=64)\n",
    "    text = \"This is a sample text to test whether it's ai generated.\"\n",
    "    print(\"AI probability:\", w.predict_proba(text))\n",
    "\n",
    "    # multiple models\n",
    "    # cps = [\"model_a.pt\", \"model_b.pt\", ...]\n",
    "    # probs = predict_from_many_models(cps, text)\n",
    "    # print(probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a7cb9ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Adapter] device -> cuda:1\n",
      "[Adapter] Checkpoint loaded. epoch=0, acc=0.0, model_name=bert-base-cased\n",
      "[Adapter] instantiating AutoModel.from_pretrained('bert-base-cased')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Adapter] full model state loaded.\n",
      "AI prob: 0.19196173548698425\n",
      "0.2858  -  This looks like an AI generated sentence.\n",
      "0.7272  -  I went to the store and bought some apples.\n",
      "0.2458  -  In conclusion, this paragraph was likely produced by a transformer.\n"
     ]
    }
   ],
   "source": [
    "# Safe loader + evaluator for your \"best_simplebert.pt\" checkpoints\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import inspect\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "class CheckpointAdapter:\n",
    "    \"\"\"\n",
    "    Loads checkpoints saved with your baseline training loop and provides\n",
    "    an isolated TransformerClassifier instance ready for evaluation on a single GPU.\n",
    "    This class now also contains a convenience 'InferenceWrapper' that loads a tokenizer\n",
    "    and provides predict_proba / predict_proba_batch methods.\n",
    "    \"\"\"\n",
    "    def __init__(self, device: Optional[str] = None):\n",
    "        if device is None:\n",
    "            self.device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "        else:\n",
    "            self.device = torch.device(device)\n",
    "        print(f\"[Adapter] device -> {self.device}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def _fix_state_dict(state_dict):\n",
    "        if not state_dict:\n",
    "            return state_dict\n",
    "        keys = list(state_dict.keys())\n",
    "        if not keys:\n",
    "            return state_dict\n",
    "        has_module = keys[0].startswith(\"module.\")\n",
    "        if has_module:\n",
    "            return {k.replace(\"module.\", \"\"): v for k, v in state_dict.items()}\n",
    "        return state_dict\n",
    "\n",
    "    class TransformerClassifier(nn.Module):\n",
    "        def __init__(self, transformer_model, hidden_size, num_labels, dropout_rate=0.1):\n",
    "            super().__init__()\n",
    "            self.transformer = transformer_model\n",
    "            self.dropout = nn.Dropout(dropout_rate)\n",
    "            self.classifier = nn.Linear(hidden_size, num_labels)\n",
    "\n",
    "        def forward(self, input_ids, attention_mask=None):\n",
    "            outputs = self.transformer(input_ids, attention_mask=attention_mask, return_dict=True)\n",
    "            hidden_states = outputs.last_hidden_state\n",
    "            if attention_mask is not None:\n",
    "                mask_expanded = attention_mask.unsqueeze(-1).expand_as(hidden_states).float()\n",
    "                sum_embeddings = torch.sum(hidden_states * mask_expanded, dim=1)\n",
    "                sum_mask = torch.clamp(mask_expanded.sum(dim=1), min=1e-9)\n",
    "                pooled = sum_embeddings / sum_mask\n",
    "            else:\n",
    "                pooled = hidden_states.mean(dim=1)\n",
    "            pooled = self.dropout(pooled)\n",
    "            return self.classifier(pooled)\n",
    "\n",
    "    def load_checkpoint(self, checkpoint_path: str, transformer_factory=None, strict_load: bool = True) -> Tuple[nn.Module, dict]:\n",
    "        \"\"\"\n",
    "        Load checkpoint and return (model, meta).\n",
    "        This is your original loader; unchanged except returning model+meta for further wrapping.\n",
    "        \"\"\"\n",
    "        if not os.path.exists(checkpoint_path):\n",
    "            raise FileNotFoundError(f\"Checkpoint not found: {checkpoint_path}\")\n",
    "        ckpt = torch.load(checkpoint_path, map_location=\"cpu\")\n",
    "        meta = {\n",
    "            'epoch': ckpt.get('epoch'),\n",
    "            'accuracy': ckpt.get('accuracy', ckpt.get('test_accuracy')),\n",
    "            'seed': ckpt.get('seed'),\n",
    "            'model_name': ckpt.get('model_name') or (ckpt.get('config', {}) or {}).get('model_name'),\n",
    "            'label_map': ckpt.get('label_map'),\n",
    "            'raw_ckpt': ckpt\n",
    "        }\n",
    "        print(f\"[Adapter] Checkpoint loaded. epoch={meta['epoch']}, acc={meta['accuracy']}, model_name={meta['model_name']}\")\n",
    "\n",
    "        # Figure out what was saved\n",
    "        full_classifier_state = ckpt.get('state_dict')\n",
    "        transformer_only_state = ckpt.get('transformer_state_dict')\n",
    "\n",
    "        # Instantiate transformer\n",
    "        if transformer_factory is not None:\n",
    "            try:\n",
    "                sig = inspect.signature(transformer_factory)\n",
    "                transformer = transformer_factory() if len(sig.parameters) == 0 else transformer_factory(meta['model_name'])\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(f\"transformer_factory failed: {e}\")\n",
    "        else:\n",
    "            if meta['model_name'] is None:\n",
    "                raise RuntimeError(\"No model_name in checkpoint and no transformer_factory provided.\")\n",
    "            print(f\"[Adapter] instantiating AutoModel.from_pretrained('{meta['model_name']}')\")\n",
    "            transformer = AutoModel.from_pretrained(meta['model_name'])\n",
    "\n",
    "        # Load transformer weights if present\n",
    "        if transformer_only_state is not None:\n",
    "            fixed_t = self._fix_state_dict(transformer_only_state)\n",
    "            missing, unexpected = transformer.load_state_dict(fixed_t, strict=False)\n",
    "            if missing or unexpected:\n",
    "                print(f\"[Adapter] (info) transformer load missing={len(missing)} unexpected={len(unexpected)}\")\n",
    "\n",
    "        hidden_size = getattr(transformer.config, \"hidden_size\", None)\n",
    "        if hidden_size is None:\n",
    "            raise RuntimeError(\"Transformer config has no hidden_size.\")\n",
    "\n",
    "        # Infer num_labels\n",
    "        num_labels = None\n",
    "        if meta['label_map']:\n",
    "            try:\n",
    "                num_labels = len(meta['label_map'])\n",
    "            except Exception:\n",
    "                pass\n",
    "        if num_labels is None and full_classifier_state:\n",
    "            for k in ['classifier.weight', 'module.classifier.weight']:\n",
    "                if k in full_classifier_state:\n",
    "                    num_labels = full_classifier_state[k].shape[0]\n",
    "                    break\n",
    "        if num_labels is None:\n",
    "            raise RuntimeError(\"Could not infer num_labels (need label_map or classifier weights).\")\n",
    "\n",
    "        model = self.TransformerClassifier(transformer, hidden_size, num_labels).to(self.device)\n",
    "\n",
    "        # If a full model (transformer + classifier) state dict was saved, load it\n",
    "        if full_classifier_state:\n",
    "            fixed = self._fix_state_dict(full_classifier_state)\n",
    "            try:\n",
    "                model.load_state_dict(fixed, strict=strict_load)\n",
    "                print(\"[Adapter] full model state loaded.\")\n",
    "            except RuntimeError as e:\n",
    "                print(f\"[Adapter] strict load failed: {e}\")\n",
    "                if strict_load:\n",
    "                    print(\"[Adapter] retrying with strict=False\")\n",
    "                    model.load_state_dict(fixed, strict=False)\n",
    "\n",
    "        model.eval()\n",
    "        return model, meta\n",
    "\n",
    "    def evaluate_model(self, model, dataloader, eval_name=\"eval\", label_list=None):\n",
    "        import time, numpy as np\n",
    "        from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "        if label_list is None and 'label_list' in globals():\n",
    "            label_list = globals()['label_list']\n",
    "        model.to(self.device).eval()\n",
    "        all_preds, all_labels = [], []\n",
    "        t0 = time.time()\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                # Expect tuple: (input_ids, attention_mask, labels, *optional)\n",
    "                input_ids = batch[0].to(self.device)\n",
    "                attention_mask = batch[1].to(self.device) if len(batch) > 1 else None\n",
    "                labels = batch[2].to(self.device) if len(batch) > 2 else None\n",
    "                logits = model(input_ids, attention_mask=attention_mask)\n",
    "                preds = torch.argmax(logits, dim=-1)\n",
    "                if labels is not None:\n",
    "                    all_labels.extend(labels.cpu().tolist())\n",
    "                all_preds.extend(preds.cpu().tolist())\n",
    "        if all_labels:\n",
    "            acc = accuracy_score(all_labels, all_preds)\n",
    "            if label_list:\n",
    "                report = classification_report(all_labels, all_preds, target_names=label_list, zero_division=0, output_dict=True)\n",
    "            else:\n",
    "                report = classification_report(all_labels, all_preds, zero_division=0, output_dict=True)\n",
    "            cm = confusion_matrix(all_labels, all_preds).tolist()\n",
    "            # Assume positive class = 1 if binary\n",
    "            if len(set(all_labels)) == 2 and 1 in set(all_labels):\n",
    "                denom = max(1, sum(1 for x in all_labels if x == 0))\n",
    "                fpr = sum(1 for p, y in zip(all_preds, all_labels) if p == 1 and y == 0) / denom\n",
    "            else:\n",
    "                fpr = None\n",
    "        else:\n",
    "            acc, report, cm, fpr = None, {}, [], None\n",
    "        print(f\"[{eval_name}] done in {time.time()-t0:.1f}s acc={acc}\")\n",
    "        return {\n",
    "            \"name\": eval_name,\n",
    "            \"accuracy\": acc,\n",
    "            \"classification_report\": report,\n",
    "            \"confusion_matrix\": cm,\n",
    "            \"false_positive_rate\": fpr\n",
    "        }\n",
    "\n",
    "    def evaluate_on_df(self, model, eval_df, eval_name=\"df_eval\", batch_size=32):\n",
    "        if 'generate_data_loader' not in globals():\n",
    "            raise RuntimeError(\"generate_data_loader not defined in this notebook.\")\n",
    "        dataloader = generate_data_loader(eval_df, batch_size=batch_size, do_shuffle=False, balance_label_examples=False)\n",
    "        return self.evaluate_model(model, dataloader, eval_name=eval_name)\n",
    "\n",
    "    # ---- New: inference wrapper builder ----\n",
    "    class InferenceWrapper:\n",
    "        \"\"\"\n",
    "        Lightweight wrapper returned by adapter.build_inference_wrapper(...)\n",
    "        provides predict_proba & predict_proba_batch.\n",
    "        \"\"\"\n",
    "        def __init__(self, model: nn.Module, meta: dict, tokenizer: AutoTokenizer, device: torch.device, max_seq_length: int = 128):\n",
    "            self.model = model.to(device)\n",
    "            self.meta = meta\n",
    "            self.tokenizer = tokenizer\n",
    "            self.device = device\n",
    "            self.max_seq_length = max_seq_length\n",
    "            self.model.eval()\n",
    "            self.ai_label_index = self._resolve_ai_index(meta.get('label_map'))\n",
    "\n",
    "        def _resolve_ai_index(self, label_map):\n",
    "            \"\"\"\n",
    "            Resolve ai label index from label_map which can be dict or list.\n",
    "            If not present, fallback to index 1 (assuming [human, ai]).\n",
    "            \"\"\"\n",
    "            if not label_map:\n",
    "                return 1\n",
    "            if isinstance(label_map, dict):\n",
    "                # prefer 'ai' key if maps name->index\n",
    "                if 'ai' in label_map:\n",
    "                    return int(label_map['ai'])\n",
    "                # otherwise try detect index->label\n",
    "                for k, v in label_map.items():\n",
    "                    if str(v).lower() == 'ai':\n",
    "                        try:\n",
    "                            return int(k)\n",
    "                        except:\n",
    "                            pass\n",
    "            if isinstance(label_map, (list, tuple)):\n",
    "                for i, v in enumerate(label_map):\n",
    "                    if str(v).lower() == 'ai':\n",
    "                        return i\n",
    "            # fallback\n",
    "            return 1\n",
    "\n",
    "        def predict_proba(self, text: str) -> float:\n",
    "            \"\"\"\n",
    "            Single-string inference. Returns probability of 'ai' label in [0,1].\n",
    "            \"\"\"\n",
    "            self.model.eval()\n",
    "            encoded = self.tokenizer(\n",
    "                text,\n",
    "                truncation=True,\n",
    "                padding='max_length',\n",
    "                max_length=self.max_seq_length,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            input_ids = encoded['input_ids'].to(self.device)\n",
    "            attention_mask = encoded['attention_mask'].to(self.device) if 'attention_mask' in encoded else None\n",
    "\n",
    "            with torch.no_grad():\n",
    "                logits = self.model(input_ids, attention_mask=attention_mask)  # shape (1, num_labels)\n",
    "                probs = F.softmax(logits, dim=-1)  # (1, num_labels)\n",
    "                idx = int(self.ai_label_index)\n",
    "                if idx < 0 or idx >= probs.shape[1]:\n",
    "                    # fallback to index 1 if available\n",
    "                    idx = 1 if probs.shape[1] > 1 else 0\n",
    "                ai_prob = probs[0, idx].item()\n",
    "            return float(ai_prob)\n",
    "\n",
    "        def predict_proba_batch(self, texts: List[str], batch_size: int = 32) -> List[float]:\n",
    "            \"\"\"\n",
    "            Batch inference, returns list of probabilities in same order as texts.\n",
    "            \"\"\"\n",
    "            results = []\n",
    "            for i in range(0, len(texts), batch_size):\n",
    "                batch_texts = texts[i:i+batch_size]\n",
    "                encoded = self.tokenizer(\n",
    "                    batch_texts,\n",
    "                    truncation=True,\n",
    "                    padding='max_length',\n",
    "                    max_length=self.max_seq_length,\n",
    "                    return_tensors='pt'\n",
    "                )\n",
    "                input_ids = encoded['input_ids'].to(self.device)\n",
    "                attention_mask = encoded['attention_mask'].to(self.device) if 'attention_mask' in encoded else None\n",
    "                with torch.no_grad():\n",
    "                    logits = self.model(input_ids, attention_mask=attention_mask)  # (B, num_labels)\n",
    "                    probs = F.softmax(logits, dim=-1)  # (B, num_labels)\n",
    "                    idx = int(self.ai_label_index)\n",
    "                    if idx < 0 or idx >= probs.shape[1]:\n",
    "                        idx = 1 if probs.shape[1] > 1 else 0\n",
    "                    batch_probs = probs[:, idx].detach().cpu().tolist()\n",
    "                    results.extend(batch_probs)\n",
    "            return [float(x) for x in results]\n",
    "\n",
    "    def build_inference_wrapper(self,\n",
    "                                checkpoint_path: str,\n",
    "                                transformer_factory=None,\n",
    "                                strict_load: bool = True,\n",
    "                                tokenizer: Optional[AutoTokenizer] = None,\n",
    "                                max_seq_length: int = 128) -> 'CheckpointAdapter.InferenceWrapper':\n",
    "        \"\"\"\n",
    "        Convenience method:\n",
    "        - loads checkpoint (model + meta)\n",
    "        - instantiates tokenizer if not provided (AutoTokenizer.from_pretrained(meta['model_name']))\n",
    "        - returns InferenceWrapper with predict_proba / predict_proba_batch\n",
    "        \"\"\"\n",
    "        model, meta = self.load_checkpoint(checkpoint_path, transformer_factory=transformer_factory, strict_load=strict_load)\n",
    "\n",
    "        if tokenizer is None:\n",
    "            model_name = meta.get('model_name')\n",
    "            if not model_name:\n",
    "                raise RuntimeError(\"Checkpoint lacks model_name; please supply `tokenizer` or `transformer_factory`.\")\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "        wrapper = CheckpointAdapter.InferenceWrapper(model=model, meta=meta, tokenizer=tokenizer, device=self.device, max_seq_length=max_seq_length)\n",
    "        return wrapper\n",
    "\n",
    "# --------------------------\n",
    "# Example usage:\n",
    "# --------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    checkpoint_path = \"best_simplebert.pt\"   # <-- update if necessary\n",
    "    adapter = CheckpointAdapter(device=None)  # will pick cuda:1 if available else cpu\n",
    "\n",
    "    # Build inference wrapper (loads tokenizer automatically using model_name from checkpoint)\n",
    "    inf = adapter.build_inference_wrapper(checkpoint_path, max_seq_length=128)\n",
    "\n",
    "    # Single text:\n",
    "    sample_text = \"This sample sentence tests whether the model thinks it's AI-written.\"\n",
    "    print(\"AI prob:\", inf.predict_proba(sample_text))\n",
    "\n",
    "    # Batch\n",
    "    texts = [\n",
    "        \"This looks like an AI generated sentence.\",\n",
    "        \"I went to the store and bought some apples.\",\n",
    "        \"In conclusion, this paragraph was likely produced by a transformer.\"\n",
    "    ]\n",
    "    probs = inf.predict_proba_batch(texts, batch_size=2)\n",
    "    for t, p in zip(texts, probs):\n",
    "        print(f\"{p:.4f}  -  {t}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e0c105",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "harshitml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
